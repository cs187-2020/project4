{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "deletable": false, "editable": false, "jupyter": {"outputs_hidden": true, "source_hidden": true}}, "outputs": [], "source": ["# Please do not change this cell because some hidden tests might depend on it.\n", "import os\n", "\n", "# Otter grader does not handle ! commands well, so we define and use our\n", "# own function to execute shell commands.\n", "def shell(commands, warn=True):\n", "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n", "     \n", "       Prints the result to stdout and returns the exit status. \n", "       Provides a printed warning on non-zero exit status unless `warn` \n", "       flag is unset.\n", "    \"\"\"\n", "    file = os.popen(commands)\n", "    print (file.read().rstrip('\\n'))\n", "    exit_status = file.close()\n", "    if warn and exit_status != None:\n", "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n", "    return exit_status\n", "\n", "shell(\"\"\"\n", "ls requirements.txt >/dev/null 2>&1\n", "if [ ! $? = 0 ]; then\n", " rm -rf .tmp\n", " git clone https://github.com/cs187-2020/project4.git .tmp\n", " mv .tmp/requirements.txt ./\n", " rm -rf .tmp\n", "fi\n", "pip install -q -r requirements.txt\n", "\"\"\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["# Initialize Otter\n", "import otter\n", "grader = otter.Notebook()"]}, {"cell_type": "raw", "metadata": {"jupyter": {"source_hidden": true}}, "source": ["%%latex\n", "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n", "\\newcommand{\\cnt}[1]{\\sharp(#1)}\n", "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n", "\\newcommand{\\softmax}{\\operatorname{softmax}}\n", "\\newcommand{\\Prob}{\\Pr}\n", "\\newcommand{\\given}{\\,|\\,}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["$$\n", "\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n", "\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n", "\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n", "\\renewcommand{\\softmax}{\\operatorname{softmax}}\n", "\\renewcommand{\\Prob}{\\Pr}\n", "\\renewcommand{\\given}{\\,|\\,}\n", "$$"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "qyLRFIJ0pWug"}, "source": ["# Project 4: Semantic Parsing for Question Answering\n", "\n", "Semantic parsing is an important task in Natural Language Processing (NLP), where the goal is to convert natural language to its logical form, such as SQL. In the last project, you have built a parsing system to extract parse trees from the questions in the ATIS dataset. However, that only solves an intermediary task, not any end-user task.\n", "\n", "In this project, you will go one step further to build a semantic parsing system to convert the questions to SQL queries, such that by consulting a database you will be able to answer those questions. You will implement both a rule-based approach and an end-to-end sequence-to-sequence (seq2seq) approach. Both algorithms come with their pros and cons, and by the end of this homework you should have a basic understanding of the characteristics of the traditional computational lingustic approach and the recent neural approach. \n", "\n", "## Goals\n", "\n", "1. Build a semantic parsing algorithm to convert text to SQL queries based on the syntactic parse trees from the last project.\n", "2. Build an end-to-end seq2seq system to convert text to SQL.\n", "3. Discuss the pros and cons of the rule-based system and the end-to-end system.\n", "\n", "This will be a very challenging homework, so we recommend you to start early."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "IZC1IGmoujpw"}, "source": ["## Setup"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "ebLelc9O0v4b"}, "outputs": [], "source": ["!pip install -q dateparser\n", "!pip install -q nltk\n", "!pip install -q cryptography\n", "!pip install -qU torchtext\n", "!pip install -q mysql-connector"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "U1jEUUYXpU0V"}, "outputs": [], "source": ["import math\n", "import copy\n", "import requests\n", "import datetime\n", "\n", "import torch\n", "import torch.nn as nn\n", "from torch.nn.utils.rnn import pack_padded_sequence as pack\n", "import torchtext as tt\n", "\n", "from tqdm import tqdm\n", "\n", "import dateparser\n", "\n", "import nltk\n", "from nltk.tree import Tree\n", "from nltk import treetransforms\n", "\n", "from cryptography.fernet import Fernet\n", "\n", "import mysql.connector\n", "from mysql.connector import errorcode\n", "\n", "# Set random seeds\n", "seed = 1234\n", "torch.manual_seed(seed)\n", "\n", "# GPU check, make sure to set runtime type to \"GPU\" where available\n", "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "print (device)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "1aPr0imd09E2"}, "outputs": [], "source": ["# Tree utils\n", "!wget -nv -N -P scripts https://raw.githubusercontent.com/nlp-course/data/master/scripts/trees/tree_utils.py\n", "!wget -nv -N -P scripts https://raw.githubusercontent.com/nlp-course/data/master/scripts/trees/tree_utils_private\n", "\n", "# Add parse_tree function from the solutions to the last segment\n", "key = '5_pggebiNGJfgNYJOlQiDRGfi1PCZeRuo6vBYDKtza8='\n", "fernet = Fernet(key)\n", "with open('scripts/tree_utils_private', 'rb') as fin:\n", "  with open('scripts/tree_utils.py', 'ab') as fout:\n", "    encrypted_data = fin.read()\n", "    fout.write('\\n'.encode())\n", "    fout.write(fernet.decrypt(encrypted_data))\n", "\n", "from scripts.tree_utils import parse_tree"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "zBALH7ZHLjhh"}, "source": ["### Load data\n", "\n", "In this segment, we only consider `flight_id`-type questions."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "AIVb--YGLVKd"}, "outputs": [], "source": ["!wget -nv -N -P data https://raw.githubusercontent.com/nlp-course/data/master/ATIS/test_flightid.nl\n", "!wget -nv -N -P data https://raw.githubusercontent.com/nlp-course/data/master/ATIS/test_flightid.sql\n", "\n", "!wget -nv -N -P data https://raw.githubusercontent.com/nlp-course/data/master/ATIS/dev_flightid.nl\n", "!wget -nv -N -P data https://raw.githubusercontent.com/nlp-course/data/master/ATIS/dev_flightid.sql\n", "  \n", "!wget -nv -N -P data https://raw.githubusercontent.com/nlp-course/data/master/ATIS/train_flightid.nl\n", "!wget -nv -N -P data https://raw.githubusercontent.com/nlp-course/data/master/ATIS/train_flightid.sql"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "UW7A_icx3zjr"}, "source": ["Let's take a look at the data: the questions are in `.nl` files, and the SQL queries are in `.sql` files. The goal of this project is to convert a question to its corresponding SQL."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "Ux_vIRC337FG"}, "outputs": [], "source": ["!head -1 data/dev_flightid.nl\n", "!head -1 data/dev_flightid.sql"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "32tFSXqiLq2h"}, "source": ["### Data preprocessing"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "GxjjnghgCJ_b"}, "source": ["We use `torchtext` to process data. We use two Fields: `TEXT` for the questions, and `SQL` for the SQL queries."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "ODU_4JYO8d9q"}, "outputs": [], "source": ["def reverse(tokens):\n", "  \"\"\"Reverse a list\"\"\"\n", "  return list(reversed(tokens))\n", "\n", "TEXT = tt.data.Field(lower=True, # lowercased\n", "                     sequential=True, # sequential data\n", "                     include_lengths=True, # include lengths\n", "                     batch_first=False, # batches will be max_len X batch_size\n", "                     tokenize=lambda x: x.split(), # use split to tokenize\n", "                     preprocessing=reverse) \n", "SQL = tt.data.Field(sequential=True,\n", "                    include_lengths=False,\n", "                    batch_first=False,\n", "                    tokenize=lambda x: x.split(),\n", "                    init_token=\"<bos>\", # prepend <bos>\n", "                    eos_token=\"<eos>\")  # append <eos>\n", "fields = [('text_reversed', TEXT), ('sql', SQL)]"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "PRXJ17mU99V0"}, "source": ["Note that we reversed the tokens in question by passing in `preprocessing=reverse`. We did that because in seq2seq (w/o attention) this trick improves performance. You can refer to Section 3.3 in [the seminal seq2seq paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43155.pdf) for more details. Another difference is that we use `batch_first=False`, such that the returned batched tensors would be of size `max_length X batch_size`, which facilitates seq2seq implementation.\n", "\n", "Now, we load data using `torchtext`. We use `TranslationDataset` class here because our task is essentially a translation task: \"translating\" questions into the corresponding SQL queries. Therefore, we also refer to the questions as the source side, the SQL queries as the target side."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "pIb_oHI33ga-"}, "outputs": [], "source": ["# Make splits for data\n", "train_data, val_data, test_data = tt.datasets.TranslationDataset.splits(\n", "    ('_flightid.nl', '_flightid.sql'), fields, path='./data/',\n", "    train='train', validation='dev', test='test')\n", "\n", "MIN_FREQ = 3\n", "TEXT.build_vocab(train_data.text_reversed, min_freq=MIN_FREQ)\n", "SQL.build_vocab(train_data.sql, min_freq=MIN_FREQ)\n", "\n", "print (f\"Size of English vocab: {len(TEXT.vocab)}\")\n", "print (f\"Most comman English words: {TEXT.vocab.freqs.most_common(10)}\")\n", "\n", "print (f\"Size of SQL vocab: {len(SQL.vocab)}\")\n", "print (f\"Most comman SQL words: {SQL.vocab.freqs.most_common(10)}\")\n", "\n", "print (f\"Start of sequence: {SQL.vocab.stoi[SQL.init_token]}\") # word id for bos\n", "print (f\"End of sequence: {SQL.vocab.stoi[SQL.eos_token]}\")   # word id for eos"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "y89Bvq7WJbc8"}, "source": ["Next, we batch our data to facilitate processing on GPU. Batching is a bit tricky because source/target will be of different lengths. Fortunately, `torchtext` allows us to pass in a `sort_key` function. This will minimize the amount of padding on the source side, but since there is still some padding, we need to handle them with [`pack`](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence) later on in the seq2seq part. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "lnoY1oIj37bC"}, "outputs": [], "source": ["BATCH_SIZE = 32 # batch size for training/validation\n", "TEST_BATCH_SIZE = 1 # batch size for test, we use 1 to make implementation easier\n", "train_iter, val_iter = tt.data.BucketIterator.splits((train_data, val_data), batch_size=BATCH_SIZE, device=device,\n", "                                                  repeat=False, sort_key=lambda x: len(x.text_reversed), sort_within_batch=True)\n", "test_iter = tt.data.BucketIterator(test_data, batch_size=1, device=device,\n", "                                                  repeat=False, sort=False, train=False)"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "kVzpFHVRKM1k"}, "source": ["Let's look at a single batch from one of these iterators."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "SeEpkHdiKVYV"}, "outputs": [], "source": ["batch = next(iter(val_iter))\n", "text, text_lengths = batch.text_reversed\n", "print (f\"Size of text batch: {text.size()}\")\n", "print (f\"Third sentence in batch: {text[:, 2]}\")\n", "print (f\"Length of the third sentence in batch: {text_lengths[2]}\")\n", "print (f\"Converted back to string: {' '.join([TEXT.vocab.itos[i] for i in text[:, 2]])}\")\n", "\n", "sql = batch.sql\n", "print (f\"Size of sql batch: {sql.size()}\")\n", "print (f\"Third label in batch: {sql[:, 2]}\")\n", "print (f\"Converted back to string: {' '.join([SQL.vocab.itos[i] for i in sql[:, 2]])}\")"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "LZe96xm-tx2-"}, "source": ["Note that the question is reversed, and that the size of the batch is `max_length X batch_size`. Alternatively, we can directly iterate over the raw examples in train_data, val_data and test_data."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "V6yIqRmwt2FA"}, "outputs": [], "source": ["for example in val_iter.dataset: # val_iter.dataset is just val_data\n", "  text_reversed = example.text_reversed\n", "  text = ' '.join(reversed(text_reversed)) # detokenized question\n", "  sql = ' '.join(example.sql) # detokenized sql\n", "  print (f\"Question: {text}\")\n", "  print (f\"SQL: {sql}\")\n", "  break"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "Vp_CdmJC5ZY2"}, "source": ["\n", "### Remote ATIS Database\n", "\n", "The output of our systems are SQL queries, but to get the actual answer, we need to execute those queries on a database. We have set up a remote MySQL database, and we will connect to it using mysql-connector later.\n"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "0Mri4_xjCecY"}, "source": ["## Rule-based Semantic Parsing\n", "\n", "First, we will implement a rule-based semantic parser using the parse trees from our last project."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "3Vw9KC-T40aP"}, "source": ["### CKY Parsing\n", "\n", "We use our parse trees from the previous segment. We provide a function `parse_tree` which returns the parse tree as an `nltk.Tree` object. `parse_tree` is able to parse about 50% of ATIS questions, and it returns `None` if a question is not parsable. For higher coverage, feel free to use your own implementation."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "93iiOXdkWv5c"}, "outputs": [], "source": ["question = 'flights to boston'\n", "tree = parse_tree(question)\n", "tree.pretty_print()"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "r9ImsH7kYGC2"}, "source": ["### Semantic Parsing: The Basics"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "-0MhfJ8jYX4s"}, "source": ["The high-level idea of rule-based semantic parsing is to associate each grammar rule with a semantic rule. Given a sentence, we first construct its parse tree, then compose semantic rules bottom-up, until eventually we arrive at the root node with a finished SQL statement. \n", "\n", "We use the above parse tree as an example. \n", "\n", "1. First, let the rule\n", "\n", "   **FLIGHT -> flights**\n", "\n", "   be accompanied by the semantic rule:\n", "\n", "   **SELECT DISTINCT flight.flight_id FROM flight**.\n", "\n", "\n", "2. To handle origin/destination constraint 'boston', we associate\n", "\n", "   **Place -> boston**\n", "\n", "   with\n", "\n", "   **(SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN (SELECT city.city_code FROM city WHERE city.city_name = 'boston'))**.\n", "\n", "   Note that we look up the airport code instead of directly using city code, because the flight table which we later use expects the airport code.\n", "\n", "3. To distinguish destination from origin, we need to add a rule for: \n", "\n", "   **PPLACE -> to**. \n", "\n", "   We use lambda calculus here, since the SQL statement it produces is dependent on its siblings ('to boston' is different from 'to dallas'):\n", "\n", "   **$\\lambda$ x. \"(flight.to_airport IN (\" + x + \"))\"**.\n", "\n", "\n", "4. Now we need to merge *PPLACE* and *PLACE* at node *PP*:\n", "\n", "   **PP -> PPLACE PLACE**. \n", "\n", "   We simply use:\n", "\n", "   **left_child(right_child)**, \n", "\n", "   which denotes evaluating the left child to get a function, then applying that function with the right child as the input. In this case, this would evaluate to:\n", "\n", "   *(flight.to_airport IN (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN (SELECT city.city_code FROM city WHERE city.city_name = 'boston')))*\n", "\n", "\n", "5. For the rule\n", "\n", "   **PPS -> PP**,\n", "\n", "   we simply copy the evaluation result of the child:\n", "\n", "   **child**.\n", "\n", "6. Finally, the last piece to complete the puzzle is at the root node:\n", "\n", "   **S -> FLIGHT PPS**,\n", "\n", "   for which we only need to join the evaluation results of its left child and right child with a 'WHERE':\n", "\n", "   **left_child WHERE right_child**.\n", "\n", "Putting all these together, the final SQL statement we get (at root 'S') is:\n", "\n", "*SELECT DISTINCT flight.flight_id FROM flight WHERE (flight.to_airport IN (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN (SELECT city.city_code FROM city WHERE city.city_name = 'boston')))*,\n", "\n", "which should return the answer to the original question when used to query a MySQL database containing relevant flight information."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "qF12SmJbDsYb"}, "source": ["### Goal 1: Construct SQL queries from a parse tree and evaluate the results"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "5iqruasFCaws"}, "source": ["Implement a rule-based semantic parsing system to successfully answer **at least 25%** of flight_id type questions in the test set."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "H8ciJheFAvml"}, "source": ["#### Starter Code<!--TODO-->\n", "\n", "We provide starter code for some functions that you will implement. \n", "\n", "\n", "*HINT: You may find it useful to use `WHERE TRUE AND (condition)` instead of `WHERE (condition)` in your queries. This way, if you want to add more conditions you can write it as such: `WHERE TRUE AND (condition1) AND (condition2)...`*"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "NE0QpqZmxfWx"}, "source": ["First, we provide a lexicon from our grammar.<!--TODO-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "iYVEu32GxfW0"}, "outputs": [], "source": ["#TODO\n", "# Lexicon\n", "lexicon = {\n", "  'ADJ': {\n", "    \"days\": set(\n", "      [\n", "        \"monday's\",\n", "        \"tuesday's\",\n", "        \"wednesday's\",\n", "        \"thursday's\",\n", "        \"friday's\",\n", "        \"saturday's\",\n", "        \"sunday's\",\n", "      ]\n", "    ),\n", "    \"availability\": set([\"available\", \"possible\"]),\n", "    \"seat_types\": set([\"first class\", \"economy\", \"thrift economy\"]),\n", "    \"price\": set([\"cheapest\", \"lowest cost\", \"least expensive\", \"most expensive\"]),\n", "    \"time\": set([\"weekday\", \"daily\", \"last\", \"first\"]),\n", "    \"attributes\": set([\"dinner\", \"transcontinental\"]),\n", "  },\n", "  'PDAY': {\n", "    \"arrive_on\": set(\n", "      [\n", "        \"returning on\",\n", "        \"arriving\",\n", "        \"arriving on\",\n", "        \"that arrive on\",\n", "        \"which arrive on\",\n", "      ]\n", "    ),\n", "    \"depart_on\": set(\n", "      [\n", "        \"on\",\n", "        \"of\",\n", "        \"for\",\n", "        \"next\",\n", "        \"the next\",\n", "        \"in the next\",\n", "        \"of next\",\n", "        \"leaving\",\n", "        \"which leave\",\n", "        \"leaving on\",\n", "      ]\n", "    ),\n", "  },\n", "  'PPLACE': {\n", "    \"dest\": set(\n", "      [\n", "        \"to\",\n", "        \"that arrive at\",\n", "        \"that arrives in\",\n", "        \"coming back to\",\n", "        \"that go to\",\n", "        \"and then to\",\n", "        \"arriving in\",\n", "        \"and arriving in\",\n", "        \"and arrive in\",\n", "        \"to arrive in\",\n", "        \"arrive in\",\n", "        \"going to\",\n", "        \"into\",\n", "        \"for\",\n", "        \"with the destination city of\",\n", "        \"arriving\",\n", "        \"goes to\",\n", "        \"flying into\",\n", "        \"goes on to\",\n", "        \"reaching\",\n", "        \"in\",\n", "        \"and then\",\n", "        \"arriving to\",\n", "      ]\n", "    ),\n", "    \"source\": set(\n", "      [\n", "        \"from\",\n", "        \"leaving\",\n", "        \"return from\",\n", "        \"leaving from\",\n", "        \"departing from\",\n", "        \"are departing from\",\n", "        \"departing\",\n", "        \"go from\",\n", "        \"going from\",\n", "        \"back from\",\n", "        \"that goes from\",\n", "        \"that departs\",\n", "        \"which leaves from\",\n", "        \"which leave\",\n", "        \"that leave\",\n", "        \"originating in\",\n", "        \"leave\",\n", "        \"out of\",\n", "        \"leaves from\",\n", "        \"to get from\"\n", "      ]\n", "    ),\n", "    \"through\": set(\n", "      [\n", "        \"via\",\n", "        \"with a stopover in\",\n", "        \"with a layover in\",\n", "        \"with a stopover at\",\n", "        \"and a stopover in\",\n", "        \"stop in\",\n", "        \"stopping in\",\n", "        \"make a stop in\",\n", "        \"with a stop in\",\n", "        \"with one stop in\",\n", "        \"go through\",\n", "        \"which go through\",\n", "        \"makes a stopover in\",\n", "        \"that stops in\",\n", "        \"that stops over in\",\n", "        \"by way of\",\n", "        \"connecting through\",\n", "        \"that will stop in\",\n", "        \"which connects in\",\n", "      ]\n", "    ),\n", "  },\n", "  'PTIME': {\n", "    \"arrive_by\": set(\n", "      [\n", "        \"that arrive before\",\n", "        \"that arrives before\",\n", "        \"arriving before\",\n", "        \"arrival by\",\n", "        \"arrives\",\n", "        \"before\",\n", "        \"departing before\",\n", "        \"that leaves before\",\n", "        \"which arrive before\",\n", "        \"by\",\n", "      ]\n", "    ),\n", "    \"arrive_at\": set(\n", "      [\n", "        \"around\",\n", "        \"that return around\",\n", "        \"that gets in around\",\n", "        \"at\",\n", "        \"arriving around\",\n", "        \"arriving about\",\n", "      ]\n", "    ),\n", "    \"arrive_after\": set([\"that arrive soon after\", \"arriving after\"]),\n", "    \"depart_at\": set(\n", "      [\n", "        \"leaving at\",\n", "        \"leaving\",\n", "        \"which leave after\",\n", "        \"leaving after\",\n", "        \"after\",\n", "        \"departing after\",\n", "        \"that depart after\",\n", "        \"departing at\",\n", "        \"are departing at\",\n", "      ]\n", "    ),\n", "    \"depart_in\": set([\"in\", \"departing in\", \"on\", \"that leaves in\"]),\n", "  },\n", "  'TIME': {\n", "    \"morning\": set(\n", "      [\n", "        \"the morning\",\n", "        \"the early am\",\n", "        \"mornings\",\n", "        \"as early as possible\",\n", "        \"earliest possible time\",\n", "        \"as soon thereafter as possible\",\n", "      ]\n", "    ),\n", "    \"afternoon\": set(\n", "      [\"the afternoon\", \"the late afternoon\", \"the day\", \"afternoons\"]\n", "    ),\n", "    \"evening\": set([\"the evening\", \"evenings\"]),\n", "  },\n", "}"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "KIpXuVinxfW3"}, "source": ["In addition to the provided lexicon, we also provide some helper functions. You will need to implement `eval_S`, which returns the SQL query based on a parse tree.<!--TODO-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "1WxNQbgOxfW4"}, "outputs": [], "source": ["#TODO\n", "def eval_S(tree):\n", "  \"\"\"\n", "  Construct the SQL query based on a parse tree.\n", "  Arguments:\n", "      tree: an nltk.Tree.\n", "  Returns:\n", "      a string of the corresponding SQL query\n", "  \"\"\"\n", "  #TODO: implement this method.\n", "  PREJ = None\n", "  DET = None\n", "  ADJS = None\n", "  FLIGHT = None\n", "  PPS = None\n", "\n", "  for child in tree:\n", "    if child.label() == \"PREJ\":\n", "      PREJ = child\n", "    elif child.label() == \"DET\":\n", "      DET = child\n", "    elif child.label() == \"ADJS\":\n", "      ADJS = child\n", "    elif child.label() == \"FLIGHT\":\n", "      FLIGHT = child\n", "    elif child.label() == \"PPS\":\n", "      PPS = child\n", "\n", "  ### Implement these Rules\n", "  # S -> (PREJ) (DET) ADJS FLIGHT PPS\n", "  # S -> (PREJ) (DET) ADJS FLIGHT\n", "  # S -> (PREJ) (DET) FLIGHT PPS\n", "  # S -> (PREJ) (DET) FLIGHT\n", "  ### YOUR CODE HERE\n", "  raise NotImplementedError\n", "\n", "def eval_FLIGHT(tree):\n", "  ### Implement these Rules\n", "  # FLIGHT -> 'flights' | 'flight' | 'to' 'fly'\n", "  ### YOUR CODE HERE\n", "  raise NotImplementedError\n", "\n", "def eval_PPS(tree):\n", "  ### Implement these Rules\n", "  # PPS -> PP\n", "  # PPS -> PP PPS\n", "  ### YOUR CODE HERE\n", "  raise NotImplementedError\n", "\n", "def eval_PP(tree):\n", "  # List of the labels of the children (e.g. ['PPLACE', 'PLACE'])\n", "  child_labels = [child.label() for child in tree]\n", "  ### Implement these Rules\n", "  # PP -> PPLACE PLACE OR PLACE\n", "  # PP -> PPLACE EITHER PLACE OR PLACE\n", "  # PP -> PPLACE PLACE\n", "  # PP -> BETWEEN PLACE AND PLACE\n", "  ### YOUR CODE HERE\n", "  raise NotImplementedError\n", "\n", "def eval_PPLACE(tree):\n", "  PPLACE_lexicon = lexicon['PPLACE']\n", "  # Join multiword phrases\n", "  val = ' '.join(tree).strip()\n", "  ### Implement these Rules\n", "  # PPLACE -> <departing>\n", "  # PPLACE -> <arriving>\n", "  # PPLACE -> <layover>\n", "  ### YOUR CODE HERE\n", "  raise NotImplementedError\n", "\n", "def eval_PLACE(tree):\n", "  # Join multiword phrases\n", "  val = ' '.join(tree)\n", "  ### Implement these Rules\n", "  # PLACE -> <city_name>\n", "  ### YOUR CODE HERE\n", "  raise NotImplementedError"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "SUFhVcwjDfF4"}, "source": ["#### Solution<!--Solution-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "MhLzgrkGibQZ"}, "outputs": [], "source": ["#Solution\n", "# Lexicon\n", "lexicon = {\n", "  'ADJ': {\n", "    \"days\": set(\n", "      [\n", "        \"monday's\",\n", "        \"tuesday's\",\n", "        \"wednesday's\",\n", "        \"thursday's\",\n", "        \"friday's\",\n", "        \"saturday's\",\n", "        \"sunday's\",\n", "      ]\n", "    ),\n", "    \"availability\": set([\"available\", \"possible\"]),\n", "    \"seat_types\": set([\"first class\", \"economy\", \"thrift economy\"]),\n", "    \"price\": set([\"cheapest\", \"lowest cost\", \"least expensive\", \"most expensive\"]),\n", "    \"time\": set([\"weekday\", \"daily\", \"last\", \"first\"]),\n", "    \"attributes\": set([\"dinner\", \"transcontinental\"]),\n", "  },\n", "  'PDAY': {\n", "    \"arrive_on\": set(\n", "      [\n", "        \"returning on\",\n", "        \"arriving\",\n", "        \"arriving on\",\n", "        \"that arrive on\",\n", "        \"which arrive on\",\n", "      ]\n", "    ),\n", "    \"depart_on\": set(\n", "      [\n", "        \"on\",\n", "        \"of\",\n", "        \"for\",\n", "        \"next\",\n", "        \"the next\",\n", "        \"in the next\",\n", "        \"of next\",\n", "        \"leaving\",\n", "        \"which leave\",\n", "        \"leaving on\",\n", "      ]\n", "    ),\n", "  },\n", "  'PPLACE': {\n", "    \"dest\": set(\n", "      [\n", "        \"to\",\n", "        \"that arrive at\",\n", "        \"that arrives in\",\n", "        \"coming back to\",\n", "        \"that go to\",\n", "        \"and then to\",\n", "        \"arriving in\",\n", "        \"and arriving in\",\n", "        \"and arrive in\",\n", "        \"to arrive in\",\n", "        \"arrive in\",\n", "        \"going to\",\n", "        \"into\",\n", "        \"for\",\n", "        \"with the destination city of\",\n", "        \"arriving\",\n", "        \"goes to\",\n", "        \"flying into\",\n", "        \"goes on to\",\n", "        \"reaching\",\n", "        \"in\",\n", "        \"and then\",\n", "        \"arriving to\",\n", "      ]\n", "    ),\n", "    \"source\": set(\n", "      [\n", "        \"from\",\n", "        \"leaving\",\n", "        \"return from\",\n", "        \"leaving from\",\n", "        \"departing from\",\n", "        \"are departing from\",\n", "        \"departing\",\n", "        \"go from\",\n", "        \"going from\",\n", "        \"back from\",\n", "        \"that goes from\",\n", "        \"that departs\",\n", "        \"which leaves from\",\n", "        \"which leave\",\n", "        \"that leave\",\n", "        \"originating in\",\n", "        \"leave\",\n", "        \"out of\",\n", "        \"leaves from\",\n", "        \"to get from\"\n", "      ]\n", "    ),\n", "    \"through\": set(\n", "      [\n", "        \"via\",\n", "        \"with a stopover in\",\n", "        \"with a layover in\",\n", "        \"with a stopover at\",\n", "        \"and a stopover in\",\n", "        \"stop in\",\n", "        \"stopping in\",\n", "        \"make a stop in\",\n", "        \"with a stop in\",\n", "        \"with one stop in\",\n", "        \"go through\",\n", "        \"which go through\",\n", "        \"makes a stopover in\",\n", "        \"that stops in\",\n", "        \"that stops over in\",\n", "        \"by way of\",\n", "        \"connecting through\",\n", "        \"that will stop in\",\n", "        \"which connects in\",\n", "      ]\n", "    ),\n", "  },\n", "  'PTIME': {\n", "    \"arrive_by\": set(\n", "      [\n", "        \"that arrive before\",\n", "        \"that arrives before\",\n", "        \"arriving before\",\n", "        \"arrival by\",\n", "        \"arrives\",\n", "        \"before\",\n", "        \"departing before\",\n", "        \"that leaves before\",\n", "        \"which arrive before\",\n", "        \"by\",\n", "      ]\n", "    ),\n", "    \"arrive_at\": set(\n", "      [\n", "        \"around\",\n", "        \"that return around\",\n", "        \"that gets in around\",\n", "        \"at\",\n", "        \"arriving around\",\n", "        \"arriving about\",\n", "      ]\n", "    ),\n", "    \"arrive_after\": set([\"that arrive soon after\", \"arriving after\"]),\n", "    \"depart_at\": set(\n", "      [\n", "        \"leaving at\",\n", "        \"leaving\",\n", "        \"which leave after\",\n", "        \"leaving after\",\n", "        \"after\",\n", "        \"departing after\",\n", "        \"that depart after\",\n", "        \"departing at\",\n", "        \"are departing at\",\n", "      ]\n", "    ),\n", "    \"depart_in\": set([\"in\", \"departing in\", \"on\", \"that leaves in\"]),\n", "  },\n", "  'TIME': {\n", "    \"morning\": set(\n", "      [\n", "        \"the morning\",\n", "        \"the early am\",\n", "        \"mornings\",\n", "        \"as early as possible\",\n", "        \"earliest possible time\",\n", "        \"as soon thereafter as possible\",\n", "      ]\n", "    ),\n", "    \"afternoon\": set(\n", "      [\"the afternoon\", \"the late afternoon\", \"the day\", \"afternoons\"]\n", "    ),\n", "    \"evening\": set([\"the evening\", \"evenings\"]),\n", "  },\n", "}"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "nqCTe6NiGbnH"}, "outputs": [], "source": ["#Solution\n", "def eval_S(tree):\n", "  \"\"\"\n", "  Construct the SQL query based on a parse tree.\n", "  Arguments:\n", "      tree: an nltk.Tree.\n", "  Returns:\n", "      a string of the corresponding SQL query\n", "  \"\"\"\n", "  PREJ = None\n", "  DET = None\n", "  ADJS = None\n", "  FLIGHT = None\n", "  PPS = None\n", "\n", "  for child in tree:\n", "    if child.label() == \"PREJ\":\n", "      PREJ = child\n", "    elif child.label() == \"DET\":\n", "      DET = child\n", "    elif child.label() == \"ADJS\":\n", "      ADJS = child\n", "    elif child.label() == \"FLIGHT\":\n", "      FLIGHT = child\n", "    elif child.label() == \"PPS\":\n", "      PPS = child\n", "\n", "  # S -> (PREJ) (DET) ADJS FLIGHT PPS\n", "  if ADJS and PPS:\n", "    return \"{} {} {}\".format(eval_FLIGHT(FLIGHT), eval_ADJS(ADJS), eval_PPS(PPS))\n", "  # S -> (PREJ) (DET) ADJS FLIGHT\n", "  elif ADJS:\n", "    return \"{} {}\".format(eval_FLIGHT(FLIGHT), eval_ADJS(ADJS))\n", "  # S -> (PREJ) (DET) FLIGHT PPS\n", "  elif PPS:\n", "    return \"{} {}\".format(eval_FLIGHT(FLIGHT), eval_PPS(PPS))\n", "  # S -> (PREJ) (DET) FLIGHT\n", "  else:\n", "    return eval_FLIGHT(FLIGHT)\n", "\n", "def eval_FLIGHT(tree):\n", "  # FLIGHT -> 'flights' | 'flight' | 'to' 'fly'\n", "  return 'SELECT DISTINCT flight.flight_id FROM flight WHERE TRUE'\n", "\n", "def eval_PREJ(tree):\n", "  # PREJ -> JUNK PREJ | JUNK\n", "  # Not relevant for semantics\n", "  return ''\n", "\n", "def eval_ADJS(tree):\n", "  # ADJS -> ADJ\n", "  if len(tree) == 1:\n", "    return eval_ADJ(tree[0])\n", "  # ADJS -> ADJ ADJS\n", "  else:\n", "    return \"{} {}\".format(eval_ADJ(tree[0]), eval_ADJS(tree[1]))\n", "\n", "def eval_ADJ(tree):\n", "  # ADJ -> <anything>\n", "  return ''\n", "\n", "def eval_DET(tree):\n", "  # DET -> 'all' 'the' | 'all' | A | 'an' | THE | 'any' | 'all' 'of' 'the'\n", "  # All words in lexicon should show all flights\n", "  # Might consider `THE` or `A` to only get one...\n", "  return ''\n", "\n", "def eval_PPS(tree):\n", "  # PPS -> PP\n", "  if len(tree) == 1:\n", "    return 'AND {}'.format(eval_PP(tree[0]))\n", "  # PPS -> PP PPS\n", "  else:\n", "    return \"AND {} {}\".format(eval_PP(tree[0]), eval_PPS(tree[1]))\n", "\n", "def eval_PP(tree):\n", "  child_labels = [child.label() for child in tree]\n", "  # PP -> PPLACE PLACE OR PLACE\n", "  if child_labels == ['PPLACE', 'PLACE', 'OR', 'PLACE']:\n", "    return '({} OR {})'.format(eval_PPLACE(tree[0])(eval_PLACE(tree[1])), eval_PPLACE(tree[0])(eval_PLACE(tree[3])))\n", "  # PP -> PPLACE EITHER PLACE OR PLACE\n", "  elif child_labels == ['PPLACE', 'EITHER', 'PLACE', 'OR', 'PLACE']:\n", "    return '({} OR {})'.format(eval_PPLACE(tree[0])(eval_PLACE(tree[2])), eval_PPLACE(tree[0])(eval_PLACE(tree[4])))\n", "  # PP -> PPLACE PLACE\n", "  elif child_labels == ['PPLACE', 'PLACE']:\n", "    return eval_PPLACE(tree[0])(eval_PLACE(tree[1]))\n", "  # PP -> BETWEEN PLACE AND PLACE\n", "  elif child_labels == ['BETWEEN', 'PLACE', 'AND', 'PLACE']:\n", "    return '(flight.from_airport IN {}) AND (flight.to_airport IN {})'.format(eval_PLACE(tree[1]), eval_PLACE(tree[3]))\n", "  # PP -> BETWEEN TIME AND TIME\n", "  elif child_labels == ['BETWEEN', 'TIME', 'AND', 'TIME']:\n", "    return '(flight.departure_time >= {} AND flight.departure_time <= {})'.format(eval_TIME(tree[1]), eval_TIME(tree[3]))\n", "  # PP -> WEEKDAY\n", "  elif child_labels == ['WEEKDAY']:\n", "    return eval_WEEKDAY(tree[0])\n", "  # PP -> TIME\n", "  elif child_labels == ['TIME']:\n", "    return eval_TIME(tree[0])\n", "  # PP -> PTIME TIME\n", "  elif child_labels == ['PTIME', 'TIME']:\n", "    return eval_PTIME(tree[0])(eval_TIME(tree[1]))\n", "  # PP -> EITHER PLACE OR PLACE | PLACE OR PLACE | PLACE | PDAY WEEKDAY | WEEKDAY TIME | PDAY WEEKDAY TIME | PDAY DATE | DATE | PAIRLINE AIRLINE | AIRCRAFT | FLIGHTTYPE | FARETYPE | PRICE | FOOD | AVAIL | POSTJ\n", "  else:\n", "    return 'TRUE'\n", "\n", "def eval_PPLACE(tree):\n", "  lex = lexicon['PPLACE']\n", "  val = ' '.join(tree).strip()\n", "  if val in lex['dest']:\n", "    return lambda dest_sql: '(flight.to_airport IN {})'.format(dest_sql)\n", "  elif val in lex['source']:\n", "    return lambda source_sql: '(flight.from_airport IN {})'.format(source_sql)\n", "  elif val in lex['through']:\n", "    return lambda through_sql: '(flight_stop.stop_airport IN {})'.format(through_sql)\n", "  else:\n", "    return lambda place: 'TRUE'\n", "\n", "def eval_PLACE(tree):\n", "  val = ' '.join(tree)\n", "  # Currently assumes PLACE is a city... may want to query for it and check and handle other cases\n", "  return \"(SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN (SELECT city.city_code FROM city WHERE city.city_name = '{}'))\".format(val.upper())\n", "\n", "def eval_PTIME(tree):\n", "  lex = lexicon['PTIME']\n", "  val = ' '.join(tree)\n", "  if val in lex['arrive_by']:\n", "    return lambda time: '(flight.arrival_time <= {})'.format(int(time.strftime('%H%M')))\n", "  elif val in lex['arrive_at']:\n", "    return lambda time: '(flight.arrival_time >= {} AND flight.arrival_time <= {})'.format(int((time - datetime.timedelta(minutes=30)).strftime('%H%M')), int((time + datetime.timedelta(minutes=30)).strftime('%H%M')))\n", "  elif val in lex['arrive_after']:\n", "    return lambda time: '(flight.arrival_time >= {})'.format(int(time.strftime('%H%M')))\n", "  elif val in lex['depart_at']:\n", "    return lambda time: '(flight.departure_time >= {} AND flight.departure_time <= {})'.format(int((time - datetime.timedelta(minutes=30)).strftime('%H%M')), int((time + datetime.timedelta(minutes=30)).strftime('%H%M')))\n", "  # Try to handle \"depart in\" by querying time\n", "  else:\n", "    return lambda time: ''\n", "\n", "def eval_TIME(tree):\n", "  child_labels = [child.label() for child in tree]\n", "  if child_labels == ['SIMPLETIME']:\n", "    return eval_SIMPLETIME(tree[0])\n", "  else:\n", "    return 'TRUE'\n", "\n", "def eval_SIMPLETIME(tree):\n", "  # uses natural langage parser for lexicon\n", "  val = ' '.join(tree)\n", "  try:\n", "    time = dateparser.parse(val)\n", "    return time\n", "  except:\n", "    return 'TRUE'\n", "\n", "def eval_WEEKDAY(tree):\n", "  child_labels = [child.label() for child in tree]\n", "  if child_labels == ['SIMPLEWEEKDAY'] or child_labels == ['A', 'SIMPLEWEEKDAY']:\n", "    return eval_SIMPLEWEEKDAY(tree[-1])\n", "  else:\n", "    return 'TRUE'\n", "\n", "def eval_SIMPLEWEEKDAY(tree):\n", "  val = ' '.join(tree)\n", "  return \"(flight.flight_days IN (SELECT days.days_code FROM days WHERE days.day_name = '{}'))\".format(val)"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "zoConst9BQSg"}, "source": ["#### Evaluation\n", "\n", "With a rule-based semantic parsing system, we can generate SQL queries given questions, and then execute those queries on a MySQL database to answer the given questions. To evaluate the performance of the system, we compare the returned results against the results of executing the ground truth queries. Note that we do not directly compare the predicted SQL queries to the gold SQL queries due to there being multiple ways of writing semantically equivalent queries."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "3T3b43eHBTts"}, "source": ["We provide a function `evaluate_accuracy` to compare the results from our generated SQL to the ground truth SQL."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "zMGUCce9BXEd"}, "outputs": [], "source": ["def evaluate_accuracy(predictions, sqls, questions=None):\n", "  \"\"\"\n", "  Evaluate accuracy by executing predictions on a remote MySQL database\n", "  and comparing returned results.\n", "  Arguments:\n", "      predictions: a list of predicted sqls or a single predicted sql.\n", "      sqls: a list of gold sql statements or a single gold sql.\n", "      questions: a list of questions or a single question. Optional.\n", "  Returns: accuracy.\n", "  \"\"\"\n", "  # Initial check for type of input\n", "  sqls = [sqls] if not isinstance(sqls, (list)) else sqls\n", "  predictions = [predictions] if not isinstance(predictions, (list)) else predictions\n", "  if questions is not None:\n", "    questions = [questions] if not isinstance(questions, (list)) else questions\n", "  else:\n", "    questions = ['N/A',] * len(sqls)\n", "  \n", "  # Connect to remote database\n", "  try:\n", "    conn = mysql.connector.connect(host='54.202.209.190', user='CS187', password='007')\n", "  except mysql.connector.Error as err:\n", "    if err.errno == errorcode.ER_ACCESS_DENIED_ERROR:\n", "      print(\"Something is wrong with your user name or password\")\n", "    else:\n", "      print(err)\n", "\n", "  c = conn.cursor()\n", "  c.execute('USE atis;')\n", "\n", "  # Evaluate each query and compare results\n", "  correct = 0\n", "  total = len(sqls)\n", "  for gold_sql, predicted_sql, question in zip(sqls, predictions, questions):\n", "    is_correct = True\n", "    if len(predicted_sql) == 0:\n", "      is_correct = False\n", "    else:\n", "      # Execute predicted sql\n", "      try:\n", "        c.execute(predicted_sql)\n", "        predicted_ret = c.fetchall()\n", "      except Exception as e:\n", "        predicted_ret = 'Syntax Error!'\n", "      # Execute gold sql\n", "      try:\n", "        c.execute(gold_sql)\n", "        gold_ret = c.fetchall()\n", "      except Exception as e:\n", "        gold_ret = 'Syntax Error!'\n", "      \n", "      if gold_ret == predicted_ret:\n", "        correct += 1\n", "      else:\n", "        is_correct = False\n", "    if not is_correct:\n", "      print (f\"\\nINCORRECT!\")\n", "      print (f\"Question: {question}\")\n", "      print (f\"Gold SQL: {gold_sql}\")\n", "      if len(predicted_sql) > 0:\n", "        print (f\"Gold Result: {gold_ret}\")\n", "      print (f\"Predicted SQL: {predicted_sql}\")\n", "      if len(predicted_sql) > 0:\n", "        print (f\"Predicted Result: {predicted_ret}\")\n", "  \n", "  conn.commit()\n", "  c.close()\n", "  conn.close()\n", "  return correct/total"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "IPR4zSqzM58Z"}, "source": ["To make development faster, we recommend starting with a few examples before running the full evaluation script."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "JZNW0pWxBiyj"}, "outputs": [], "source": ["# Example 1\n", "question = 'flights from phoenix to milwaukee'\n", "gold_sql = \"SELECT DISTINCT flight_1.flight_id FROM flight flight_1 , airport_service airport_service_1 , city city_1 , airport_service airport_service_2 , city city_2 WHERE flight_1.from_airport = airport_service_1.airport_code AND airport_service_1.city_code = city_1.city_code AND city_1.city_name = 'PHOENIX' AND flight_1.to_airport = airport_service_2.airport_code AND airport_service_2.city_code = city_2.city_code AND city_2.city_name = 'MILWAUKEE'\"\n", "tree = parse_tree(question)\n", "tree.pretty_print()\n", "\n", "predicted_sql = eval_S(tree)\n", "print (f\"Accuracy: {evaluate_accuracy(predicted_sql, gold_sql, question)}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "_ER7QlkPK4pg"}, "outputs": [], "source": ["# Example 2\n", "question = 'i would like a flight between boston and dallas'\n", "gold_sql = \"SELECT DISTINCT flight.flight_id FROM flight WHERE TRUE AND (flight.from_airport IN (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN (SELECT city.city_code FROM city WHERE city.city_name = 'BOSTON'))) AND (flight.to_airport IN (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN (SELECT city.city_code FROM city WHERE city.city_name = 'DALLAS')))\"\n", "tree = parse_tree(question)\n", "tree.pretty_print()\n", "\n", "predicted_sql = eval_S(tree)\n", "print (f\"Accuracy: {evaluate_accuracy(predicted_sql, gold_sql, question)}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "AJQ58JuHK6_S"}, "outputs": [], "source": ["# Example 3\n", "question = 'what flights are departing from houston or austin leaving at 7am sunday'\n", "gold_sql = \"SELECT DISTINCT flight.flight_id FROM flight WHERE TRUE AND TRUE AND ((flight.from_airport IN (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN (SELECT city.city_code FROM city WHERE city.city_name = 'HOUSTON'))) OR (flight.from_airport IN (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN (SELECT city.city_code FROM city WHERE city.city_name = 'AUSTIN')))) AND (flight.departure_time >= 630 AND flight.departure_time <= 730) AND (flight.flight_days IN (SELECT days.days_code FROM days WHERE days.day_name = 'sunday'))\"\n", "tree = parse_tree(question)\n", "tree.pretty_print()\n", "\n", "predicted_sql = eval_S(tree)\n", "print (f\"Accuracy: {evaluate_accuracy(predicted_sql, gold_sql, question)}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "8BW7LSCcK84G"}, "outputs": [], "source": ["# Example 4\n", "question = 'can i have a flight from san francisco that stops in dallas going to new york arriving before 6pm'\n", "gold_sql = \"SELECT DISTINCT flight.flight_id FROM flight WHERE TRUE AND (flight.from_airport IN (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN (SELECT city.city_code FROM city WHERE city.city_name = 'SAN FRANCISCO'))) AND (flight_stop.stop_airport IN (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN (SELECT city.city_code FROM city WHERE city.city_name = 'DALLAS'))) AND (flight.to_airport IN (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN (SELECT city.city_code FROM city WHERE city.city_name = 'NEW YORK'))) AND (flight.arrival_time <= 1800)\"\n", "tree = parse_tree(question)\n", "tree.pretty_print()\n", "\n", "predicted_sql = eval_S(tree)\n", "print (f\"Accuracy: {evaluate_accuracy(predicted_sql, gold_sql, question)}\")"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "xA3uatiaBX_f"}, "source": ["Below is the full evaluation code. Note that you are required to get correct results on **at least 25%** of flight_id type questions from the test set."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "HlO7riRGM3BT"}, "outputs": [], "source": ["questions = []\n", "predictions = []\n", "gold_sqls = []\n", "\n", "for example in test_iter.dataset:\n", "  # Input and output\n", "  text_reversed = example.text_reversed\n", "  question = ' '.join(reversed(text_reversed)) # detokenized question\n", "  gold_sql = ' '.join(example.sql) # detokenized sql\n", "  questions.append(question)\n", "  gold_sqls.append(gold_sql)\n", "  # Get parse tree\n", "  tree = parse_tree(question)\n", "  if tree is None:\n", "    predictions.append('')\n", "    continue\n", "  # Predict\n", "  try:\n", "    predicted_sql = eval_S(tree)\n", "  except Exception as e:\n", "    predictions.append('')\n", "    continue\n", "  predictions.append(predicted_sql)\n", "\n", "evaluate_accuracy(predictions, gold_sqls, questions)"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "j00wMxHP3MnB"}, "source": ["## End-to-End Seq2Seq Model"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "hE17C1s0QhSq"}, "source": ["Nowadays neural networks dominate the field of NLP research. In this part, we investigate if it is possible to use an end-to-end system to directly learn the mapping from the natural language questions to the SQL queries."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "dGvpHlNnN3ym"}, "source": ["### Goal 2: Implement a seq2seq model"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "4TPBjDalLzXg"}, "source": ["#### Model, Optimization and Decoding\n", "\n", "For the sequence-to-sequence model, you need to implement the class `EncoderDecoder`. We have provided starter code for performing optimization, but there are at least five methods that you need to implement:\n", "\n", "1. `__init__`: an initializer where you can create network modules.\n", "\n", "2. `forward`: given question word ids of size `batch_size X max_length`, question lengths of size `batch_size` and SQL word ids `batch_size X max_length_sql`, returns logits `batch_size X max_length_sql`. Note that here the batch size can be greater than 1.\n", "\n", "3. `compute_loss`: computes loss by comparing output returned by forward to ground_truth which stores the true SQL word ids.\n", "\n", "4. `evaluate_ppl`: evaluate the current model's perplexity on a given dataset iterator. [Perplexity](https://en.wikipedia.org/wiki/Perplexity) is defined as $\\exp(-\\frac{\\text{total log likelihood})}{\\text{total number of words}})$, which can be roughly understood as how many random guesses the model needs to make to get a word correct.\n", "\n", "5. `predict`: Generates the target sequence (SQL) given the source sequence (question). Note that here you can assume the batch size to be always 1 for simplicity. Besides, you can use greedy decoding here, i.e., predicting the word with the highest probability at any time step, although in practice researchers use more complicated decoding methods such as beam search. \n", "\n", "This implementation is essentially building an entire neural seq2seq system, so expect it to be very challenging. The code you write here can also be used for other seq2seq tasks such as machine translation and document summarization.\n", "\n", "*Hint: to handle source side paddings in `torch`, you can use somethine like `packed_src = pack(src, src_lengths)`. To handle target side paddings, you can use `ignore_index` when creating the loss function."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "afAaAkJXMhO7"}, "outputs": [], "source": ["#TODO\n", "class EncoderDecoder(nn.Module):\n", "  def __init__(self, text, sql, embedding_size=512, hidden_size=512, layers=2,\n", "               dropout=0, bidirectional=False, share_decoder_input_output_embeds=False,\n", "               add_encoder_out_to_decoder_input=False):\n", "    \"\"\"\n", "    Initializer. Creates network modules and loss function. You do not need to\n", "    implement all features as long as you can achieve 30%+ accuracy.\n", "    Arguments:\n", "        text: text field\n", "        tag: sql field\n", "        embedding_size: word embedding size\n", "        hidden_size: hidden layer size\n", "        layers: number of layers\n", "        dropout: dropout\n", "        bidirectional: use bidirectional RNN cells\n", "        share_decoder_input_output_embeds: if True, set the weight matrix of the \n", "            final projection layer to be the same as decoder word embeddings.\n", "            This reduces the number of parameters and is found to improve performance.\n", "            See https://arxiv.org/pdf/1608.05859.pdf.\n", "        add_encoder_out_to_decoder_input: if True, add encoder output to every\n", "            step of decoder input. This trick keeps the decoder from forgetting\n", "            encoder outputs as it decodes.\n", "    \"\"\"\n", "    super(EncoderDecoder, self).__init__()\n", "    self.text = text\n", "    self.sql = sql\n", "    # Keep the vocabulary sizes available\n", "    self.V_src = len(text.vocab.itos)\n", "    self.V_tgt = len(sql.vocab.itos)\n", "    # Get special word ids or tokens\n", "    self.padding_id_src = text.vocab.stoi[text.pad_token]\n", "    self.padding_id_tgt = sql.vocab.stoi[sql.pad_token]\n", "    self.bos_id = sql.vocab.stoi[sql.init_token]\n", "    self.eos_id = sql.vocab.stoi[sql.eos_token]\n", "    self.eos_token = sql.eos_token\n", "\n", "    # Keep parameters available\n", "    self.embedding_size = embedding_size\n", "    self.hidden_size = hidden_size\n", "    self.layers = layers\n", "    self.dropout = dropout\n", "    self.share_decoder_input_output_embeds = share_decoder_input_output_embeds\n", "    self.bidirectional = bidirectional\n", "    self.add_encoder_out_to_decoder_input = add_encoder_out_to_decoder_input\n", "\n", "    #TODO: implement this method\n", "    # Create essential modules and loss function\n", "    \"your code here\"\n", "\n", "  def forward(self, src_words, src_lengths, tgt_words):\n", "    \"\"\"\n", "    Performs forward computation, returns logits.\n", "    Arguments:\n", "        src_words: question batch of size batch_size X max_length\n", "        src_lengths: question lengths of size batch_size\n", "        tgt_words: sql batch of size batch_size X max_length\n", "    \"\"\"\n", "    #TODO: implement this method\n", "    \"your code here\"\n", "    return logits\n", "\n", "  def compute_loss(self, logits, targets):\n", "    \"\"\"\n", "    Computes loss function with logits and target.\n", "    Arguments:\n", "        logits: tensor of size batch_size X max_length X V_tgt\n", "        targets: tensor of size batch_size X max_length\n", "    \"\"\"\n", "    #TODO: implement this method\n", "    \"your code here\"\n", "    return loss\n", "\n", "  def evaluate_ppl(self, iterator):\n", "    \"\"\"\n", "    Returns the model's perplexity on a given dataset `iterator`. We will\n", "    use it for model selection.\n", "    \"\"\"\n", "    # Switch to eval mode\n", "    self.eval()\n", "    #TODO: implement this method\n", "    \"your code here\"\n", "    return perplexity\n", "\n", "  def predict(self, src_words, src_lengths, max_tgt_length=200):\n", "    \"\"\"\n", "    Generates the target sequence (SQL) given the source sequence (question).\n", "    You only need to implemnt greedy decoding, i.e., at each decoding step,\n", "    find the word with the highest probability.\n", "    Note that for simplicity, we only use batch size 1.\n", "    Arguments:\n", "        src_words: a tensor of size (max_length, 1) storing question word ids.\n", "        src_lengths: a tensor of size (1) storing question length.\n", "        max_tgt_length: at most proceed this many steps of decoding\n", "    Returns: \n", "        a string of the generated SQL.\n", "    \"\"\"\n", "    # Switch to eval mode\n", "    self.eval()\n", "    #TODO: implement this method\n", "    \"your code here\"\n", "    decoded = 'SELECT DISTINCE * FROM flight'\n", "    return decoded\n", "    \n", "  def fit(self, train_iter, val_iter, epochs=50, learning_rate=3e-4):\n", "    \"\"\"Train the model.\"\"\"\n", "    # Switch the module to training mode\n", "    self.train()\n", "    # Use Adam to optimize the parameters\n", "    optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n", "    best_validation_ppl = float('inf')\n", "    best_model = None\n", "    # Run the optimization for multiple epochs\n", "    for epoch in range(epochs): \n", "      total_words = 0\n", "      total_loss = 0.0\n", "      for batch in tqdm(train_iter):\n", "        # Zero the parameter gradients\n", "        self.zero_grad()\n", "\n", "        # Input and target\n", "        text, text_lengths = batch.text_reversed # text: max_length_text, bsz\n", "        sql = batch.sql # max_length_sql, bsz\n", "        sql_in = sql[:-1] # Remove <eos> for decode input\n", "        sql_out = sql[1:] # Remove <bos> as target\n", "        batch_size = sql.size(1)\n", "        \n", "        # Run forward pass and compute loss along the way.\n", "        logits = self.forward(text, text_lengths, sql_in)\n", "        loss = self.compute_loss(logits, sql_out)\n", "\n", "        # Training stats\n", "        num_sql_words = sql_out.ne(self.padding_id_tgt).float().sum().item()\n", "        total_words += num_sql_words\n", "        total_loss += loss.item()\n", "        \n", "        # Perform backpropagation\n", "        loss.div(batch_size).backward()\n", "        optim.step()\n", "\n", "      # Evaluate and track improvements on the validation dataset\n", "      validation_ppl = self.evaluate_ppl(val_iter)\n", "      self.train()\n", "      if validation_ppl < best_validation_ppl:\n", "        best_validation_ppl = validation_ppl\n", "        self.best_model = copy.deepcopy(self.state_dict())\n", "      epoch_loss = total_loss / total_words\n", "      print (f'Epoch: {epoch} Training Perplexity: {math.exp(epoch_loss):.4f} '\n", "             f'Validation Perplexity: {validation_ppl:.4f}')"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "PV2NuIBxOH3-"}, "source": ["#### Solution<!--Solution-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "febNSFFp4FT1"}, "outputs": [], "source": ["#Solution\n", "class EncoderDecoder(nn.Module):\n", "  def __init__(self, text, sql, embedding_size=512, hidden_size=512, layers=2,\n", "               dropout=0, bidirectional=False, share_decoder_input_output_embeds=False,\n", "               add_encoder_out_to_decoder_input=False):\n", "    \"\"\"\n", "    Initializer. Creates network modules and loss function. You do not need to\n", "    implement all features as long as you can achieve 30%+ accuracy.\n", "    Arguments:\n", "        text: text field\n", "        tag: sql field\n", "        embedding_size: word embedding size\n", "        hidden_size: hidden layer size\n", "        layers: number of layers\n", "        dropout: dropout\n", "        bidirectional: use bidirectional RNN cells\n", "        share_decoder_input_output_embeds: if True, set the weight matrix of the \n", "            final projection layer to be the same as decoder word embeddings.\n", "            This reduces the number of parameters and is found to improve performance.\n", "            See https://arxiv.org/pdf/1608.05859.pdf.\n", "        add_encoder_out_to_decoder_input: if True, add encoder output to every\n", "            step of decoder input. This trick keeps the decoder from forgetting\n", "            encoder outputs as it decodes.\n", "    \"\"\"\n", "    super(EncoderDecoder, self).__init__()\n", "    self.text = text\n", "    self.sql = sql\n", "    # Keep the vocabulary sizes available\n", "    self.V_src = len(text.vocab.itos)\n", "    self.V_tgt = len(sql.vocab.itos)\n", "    # Get special word ids or tokens\n", "    self.padding_id_src = text.vocab.stoi[text.pad_token]\n", "    self.padding_id_tgt = sql.vocab.stoi[sql.pad_token]\n", "    self.bos_id = sql.vocab.stoi[sql.init_token]\n", "    self.eos_id = sql.vocab.stoi[sql.eos_token]\n", "    self.eos_token = sql.eos_token\n", "\n", "    # Keep parameters available\n", "    self.embedding_size = embedding_size\n", "    self.hidden_size = hidden_size\n", "    self.layers = layers\n", "    self.dropout = dropout\n", "    self.share_decoder_input_output_embeds = share_decoder_input_output_embeds\n", "    self.bidirectional = bidirectional\n", "    self.add_encoder_out_to_decoder_input = add_encoder_out_to_decoder_input\n", "\n", "    # Create essential modules\n", "    self.word_embeddings_src = nn.Embedding(self.V_src, embedding_size)\n", "    self.word_embeddings_tgt = nn.Embedding(self.V_tgt, embedding_size)\n", "    self.dropout_layer = nn.Dropout(dropout)\n", "\n", "    # RNN cells\n", "    self.encoder_rnn = nn.LSTM(\n", "      input_size    = embedding_size,\n", "      hidden_size   = hidden_size//2 if bidirectional else hidden_size,\n", "      num_layers    = layers,\n", "      dropout       = dropout,\n", "      bidirectional = bidirectional\n", "    )\n", "    self.decoder_rnn = nn.LSTM(\n", "      input_size    = embedding_size,\n", "      hidden_size   = hidden_size,\n", "      num_layers    = layers,\n", "      dropout       = dropout,\n", "    )\n", "\n", "    # Final projection layer\n", "    self.hidden2output = nn.Linear(hidden_size, self.V_tgt)\n", "    if share_decoder_input_output_embeds:\n", "      self.hidden2output.weight = self.word_embeddings_tgt.weight\n", "   \n", "    # Create loss function\n", "    self.loss_function = nn.CrossEntropyLoss(reduction='sum', \n", "                                             ignore_index=self.padding_id_tgt)\n", "\n", "  def encode(self, src_words, src_lengths):\n", "    \"\"\"Encode source words into a vector\"\"\"\n", "    # Compute word embeddings\n", "    src = self.word_embeddings_src(src_words) # max_len, bsz, embedding_size\n", "    if isinstance(src_lengths, torch.LongTensor) \\\n", "            or isinstance(src_lengths, torch.cuda.LongTensor):\n", "      src_lengths = src_lengths.tolist()\n", "    # Deal with paddings\n", "    packed_src = pack(src, src_lengths)\n", "    # Forward RNN and return final state\n", "    encoder_out = self.encoder_rnn(packed_src)[-1] # num_layers*num_directions, bsz, hidden_size/num_directions\n", "    # Reshape encoder_out for bidirectional case\n", "    if self.bidirectional:\n", "      batch_size = len(src_lengths)\n", "      h, c = encoder_out\n", "      h = h.view(-1, 2, batch_size, self.hidden_size//2) \\\n", "           .transpose(1, 2) \\\n", "           .contiguous().view(-1, batch_size, self.hidden_size) # num_layers, bsz, hidden_size\n", "      c = c.view(-1, 2, batch_size, self.hidden_size//2) \\\n", "           .transpose(1, 2) \\\n", "           .contiguous().view(-1, batch_size, self.hidden_size) # num_layers, bsz, hidden_size\n", "      encoder_out = (h, c)\n", "    return encoder_out\n", "\n", "  def decode(self, tgt_words, encoder_out, feed_decoder_input):\n", "    \"\"\"Decode based on encoder output\"\"\"\n", "    # Compute word embeddings\n", "    tgt = self.word_embeddings_tgt(tgt_words) # len, bsz, hidden\n", "    # Optionally add feed_decoder_input to every step\n", "    if feed_decoder_input is not None: # bsz, hidden\n", "      tgt = tgt + feed_decoder_input.unsqueeze(0) # unsqueeze to 1, bsz, hidden\n", "    # Forward decoder RNN and return all hidden states\n", "    return self.decoder_rnn(tgt, encoder_out)[0]\n", "\n", "  def forward(self, src_words, src_lengths, tgt_words):\n", "    \"\"\"\n", "    Performs forward computation, returns logits.\n", "    Arguments:\n", "        src_words: question batch of size batch_size X max_length\n", "        src_lengths: question lengths of size batch_size\n", "        tgt_words: sql batch of size batch_size X max_length\n", "    \"\"\"\n", "    # Forward encoder\n", "    encoder_out = self.encode(src_words, src_lengths) # tuple of (h_final, c_final)\n", "    if self.share_decoder_input_output_embeds:\n", "      # h_final/c_final size: num_layers, bsz, hidden_size\n", "      # We only take the last layer to match shape of decoder inputs\n", "      feed_decoder_input = encoder_out[0][-1] + encoder_out[1][-1] # bsz, hidden_size\n", "    else:\n", "      feed_decoder_input = None\n", "    # Forward decoder\n", "    decoder_out = self.decode(tgt_words, encoder_out, feed_decoder_input)\n", "    # Final projection to target vocabulary\n", "    logits = self.hidden2output(self.dropout_layer(decoder_out))\n", "    return logits\n", "\n", "  def compute_loss(self, logits, targets):\n", "    \"\"\"\n", "    Computes loss function with logits and target.\n", "    Arguments:\n", "        logits: tensor of size batch_size X max_length X V_tgt\n", "        targets: tensor of size batch_size X max_length\n", "    \"\"\"\n", "    return self.loss_function(logits.view(-1, self.V_tgt), targets.view(-1))\n", "\n", "  def evaluate_ppl(self, iterator):\n", "    \"\"\"Returns the model's perplexity on a given dataset `iterator`.\"\"\"\n", "    # Switch to eval mode\n", "    self.eval()\n", "    total_loss = 0\n", "    total_words = 0\n", "    for batch in iterator:\n", "      # Input and target\n", "      text, text_lengths = batch.text_reversed\n", "      sql = batch.sql # max_length_sql, bsz\n", "      sql_in = sql[:-1] # remove <eos> for decode input\n", "      sql_out = sql[1:] # remove <bos> as target\n", "      # Forward to get logits\n", "      logits = self.forward(text, text_lengths, sql_in)\n", "      # Compute cross entropy loss\n", "      loss = self.compute_loss(logits, sql_out)\n", "      total_loss += loss.item()\n", "      total_words += sql_out.ne(self.padding_id_tgt).float().sum().item()\n", "    return math.exp(total_loss/total_words)\n", "\n", "  def predict(self, src_words, src_lengths, max_tgt_length=200):\n", "    \"\"\"\n", "    Generates the target sequence (SQL) given the source sequence (question).\n", "    You only need to implemnt greedy decoding, i.e., at each decoding step,\n", "    find the word with the highest probability.\n", "    Note that for simplicity, we only use batch size 1.\n", "    Arguments:\n", "        src_words: a tensor of size (max_length, 1) storing question word ids.\n", "        src_lengths: a tensor of size (1) storing question length.\n", "        max_tgt_length: at most proceed this many steps of decoding\n", "    Returns: \n", "        a string of the generated SQL.\n", "    \"\"\"\n", "    # Switch to eval mode\n", "    self.eval()\n", "    # Forward encoder\n", "    encoder_out = self.encode(src_words, src_lengths) # tuple of (h_final, c_final)\n", "    if self.share_decoder_input_output_embeds:\n", "      # h_final/c_final size: num_layers, bsz, hidden_size\n", "      # We only take the last layer to match shape of decoder inputs\n", "      feed_decoder_input = encoder_out[0][-1] + encoder_out[1][-1] # bsz, hidden_size\n", "    else:\n", "      feed_decoder_input = None\n", "    \n", "    batch_size = src_words.size(1)\n", "    # Create initial decoder input\n", "    initial_words = torch.zeros(1, batch_size, device=device).fill_(self.bos_id).long()\n", "    decoder_input = self.word_embeddings_tgt(initial_words) # 1, bsz, embedding_size\n", "    hidden = encoder_out # initialize decoder hidden state\n", "    \n", "    decoded = [] # stores partial decoding results\n", "    # Forward one step at a time\n", "    for _ in range(max_tgt_length):\n", "      # Forward decoder for one step\n", "      if self.add_encoder_out_to_decoder_input:\n", "        decoder_input = decoder_input + feed_decoder_input.unsqueeze(0)\n", "      output, hidden = self.decoder_rnn(decoder_input, hidden)\n", "      # Forward final projection\n", "      logits = self.hidden2output(self.dropout_layer(output)).squeeze(0) # bsz, vocab\n", "      # Take argmax to find the most probable word\n", "      current_words = logits.argmax(1) # bsz\n", "      # Set next step decoder inputs\n", "      words = current_words.view(1, -1)\n", "      decoder_input = self.word_embeddings_tgt(words)\n", "      # Break if eos is encountered\n", "      if current_words.item() == self.eos_id:\n", "        break\n", "      # Find the tokens\n", "      decoded.append(self.sql.vocab.itos[current_words.item()])\n", "    return ' '.join(decoded)\n", "\n", "  def fit(self, train_iter, val_iter, epochs=10, learning_rate=3e-4):\n", "    \"\"\"Train the model.\"\"\"\n", "    # Switch the module to training mode\n", "    self.train()\n", "    # Use Adam to optimize the parameters\n", "    optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n", "    best_validation_ppl = float('inf')\n", "    best_model = None\n", "    # Run the optimization for multiple epochs\n", "    for epoch in range(epochs): \n", "      total_words = 0\n", "      total_loss = 0.0\n", "      for batch in tqdm(train_iter):\n", "        # Zero the parameter gradients\n", "        self.zero_grad()\n", "\n", "        # Input and target\n", "        text, text_lengths = batch.text_reversed # text: max_length_text, bsz\n", "        sql = batch.sql # max_length_sql, bsz\n", "        sql_in = sql[:-1] # Remove <eos> for decode input\n", "        sql_out = sql[1:] # Remove <bos> as target\n", "        batch_size = sql.size(1)\n", "        \n", "        # Run forward pass and compute loss along the way.\n", "        logits = self.forward(text, text_lengths, sql_in)\n", "        loss = self.compute_loss(logits, sql_out)\n", "\n", "        # Training stats\n", "        num_sql_words = sql_out.ne(self.padding_id_tgt).float().sum().item()\n", "        total_words += num_sql_words\n", "        total_loss += loss.item()\n", "        \n", "        # Perform backpropagation\n", "        loss.div(batch_size).backward()\n", "        optim.step()\n", "\n", "      # Evaluate and track improvements on the validation dataset\n", "      validation_ppl = self.evaluate_ppl(val_iter)\n", "      self.train()\n", "      if validation_ppl < best_validation_ppl:\n", "        best_validation_ppl = validation_ppl\n", "        self.best_model = copy.deepcopy(self.state_dict())\n", "      epoch_loss = total_loss / total_words\n", "      print (f'Epoch: {epoch} Training Perplexity: {math.exp(epoch_loss):.4f} '\n", "             f'Validation Perplexity: {validation_ppl:.4f}')"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "JLuzTt-RFgoL"}, "source": ["After implementing the `EncoderDecoder` class, you can use the below script to create the model and kick off training. You are free to tune the hyperparameters."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "t4fGKij64rkQ"}, "outputs": [], "source": ["EPOCHS = 10 # epochs, we highly recommend starting with a smaller number like 1\n", "LEARNING_RATE = 3e-4 # learning rate\n", "# Instantiate and train classifier\n", "model = EncoderDecoder(TEXT, SQL,\n", "  embedding_size = 1024,\n", "  hidden_size    = 1024,\n", "  dropout        = 0.1,\n", "  layers         = 3,\n", "  bidirectional  = True,\n", "  share_decoder_input_output_embeds = True,\n", "  add_encoder_out_to_decoder_input = True,\n", ").to(device)\n", "\n", "model.fit(train_iter, val_iter, epochs=EPOCHS, learning_rate=LEARNING_RATE)\n", "model.load_state_dict(model.best_model)\n", "\n", "# Evaluate model performance, the expected value shall be < 1.3\n", "# We use validation set because this particular test set has a different distribution\n", "print (f'Validation perplexity: {model.evaluate_ppl(val_iter):.3f}')"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "dKkyL41z9ggD"}, "source": ["#### Evaluation"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "kLyrRJPsqPEs"}, "source": ["Now we are ready to run the full evaluation. For seq2seq, a proper implementation should reach at least 30% accuracy."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "vesYcMfF8XYz"}, "outputs": [], "source": ["questions = []\n", "predictions = []\n", "gold_sqls = []\n", "\n", "for example in test_iter.dataset: # val_iter.dataset is just val_data\n", "  # Input and output\n", "  text_reversed_str = example.text_reversed\n", "  question = ' '.join(list(reversed(text_reversed_str))) # detokenized question\n", "  gold_sql = ' '.join(example.sql) # detokenized sql\n", "  questions.append(question)\n", "  gold_sqls.append(gold_sql)\n", "  # Predict\n", "  text, text_lengths = TEXT.process([text_reversed_str])\n", "  text = text.to(device)\n", "  text_lengths = text_lengths.to(device)\n", "  prediction = model.predict(text, text_lengths)\n", "  print (prediction)\n", "  predictions.append(prediction)\n", "  \n", "evaluate_accuracy(predictions, gold_sqls, questions)"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "HV5Q1nuCReVv"}, "source": ["## Discussion"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "fwg23wdz1o8o"}, "source": ["### Goal 3: Compare the pros and cons of rule-based and neural approaches.\n", "\n", "Compare the pros and cons of both approaches with relevant examples from your experiments above. Concerning the accuracy, which approach would you choose to be used in a product? Explain."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "JEmuLudUbnUL"}, "source": ["#### Solution<!--Solution-->\n", "For rule-based semantic parsing, as long as the written semantic rules consider all possible cases (which is a nontrivial task), it can solve this task nicely. We list some pros and cons of this approach, but our answer is by no means exhaustive.\n", "\n", "Pros\n", "*   Clearly interpretable. When the system makes a mistake we can easily pinpoint where the problem is, and write more rules to fix it.\n", "*   Robust. For the cases that we considered, even if at test time there are examples with many constraints, the generated SQL would still be correct.\n", "*   Low sample complexity. Developing the semantic rules does not need thousands of examples. We are very good at generalization and we only used dozens of examples to write those rules in the solution.\n", "\n", "Cons\n", "*   High develop cost. It is a lot of work to develop those semantic rules.\n", "*   Poor transferability. For a new domain such as question answering in wikipedia, we need to develop a new set of rules to make this method work.\n", "\n", "For the end-to-end seq2seq approach, as long as we have enough data (which is not always the case in reality), enough model capacity (limited by hardware and time), and if the test domain is similar to the training domain, then the approach would be expeced to work well. Below lists some of its pros and cons.\n", "\n", "Pros\n", "*   High performance. With enough training data, this approach performs well as evidenced by this project.\n", "*   Low develop cost. Developing the seq2seq model is much easier compared to writing semantic rules and does not require lingustic background.\n", "*   High transferability if we have training data. For a new domain, as long as we have enough training instances, we can train the same model on the new training set to solve the problem. However, we do want to note that without training anew the model trained on one domain is unlikely to work on another.\n", "\n", "Cons\n", "*   Poor interpretability. When the model makes a mistake, there is no easy way of fixing it. The best we can do is to collect more data similar to the broken ones and add to the training set.\n", "*   High sample complexity. We need a huge training set to make this approach work. There's no way it'd work using dozens of training examples.\n", "*   Sensitive. By sensitive we meant if training set only contains compositions up to a certain level, then at test time the trained model is unlikely to work on any instance with a higher number of compositions. If we trained on sentences with length up to 100, then at test time it cannot work on sentences of length 150.\n", "\n", "Best approach:\n", "If we only care about performance, it is most natural to select the seq2seq approach due to its higher performance. Though depending on the results from precision and recall, it may be best to choose the approach with the best precision scores when applying the approach for customer use (take in the case of Alexa).\n"]}], "metadata": {"accelerator": "GPU", "colab": {"collapsed_sections": [], "include_colab_link": true, "name": "project4_semantics.ipynb", "provenance": [], "toc_visible": true}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 0}