{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "deletable": false, "editable": false, "jupyter": {"outputs_hidden": true, "source_hidden": true}}, "outputs": [], "source": ["# Please do not change this cell because some hidden tests might depend on it.\n", "import os\n", "\n", "# Otter grader does not handle ! commands well, so we define and use our\n", "# own function to execute shell commands.\n", "def shell(commands, warn=True):\n", "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n", "     \n", "       Prints the result to stdout and returns the exit status. \n", "       Provides a printed warning on non-zero exit status unless `warn` \n", "       flag is unset.\n", "    \"\"\"\n", "    file = os.popen(commands)\n", "    print (file.read().rstrip('\\n'))\n", "    exit_status = file.close()\n", "    if warn and exit_status != None:\n", "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n", "    return exit_status\n", "\n", "shell(\"\"\"\n", "ls requirements.txt >/dev/null 2>&1\n", "if [ ! $? = 0 ]; then\n", " rm -rf .tmp\n", " git clone https://github.com/cs187-2020/project4.git .tmp\n", " mv .tmp/requirements.txt ./\n", " rm -rf .tmp\n", "fi\n", "pip install -q -r requirements.txt\n", "\"\"\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["# Initialize Otter\n", "import otter\n", "grader = otter.Notebook()"]}, {"cell_type": "raw", "metadata": {}, "source": ["%%latex\n", "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n", "\\newcommand{\\cnt}[1]{\\sharp(#1)}\n", "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n", "\\newcommand{\\softmax}{\\operatorname{softmax}}\n", "\\newcommand{\\Prob}{\\Pr}\n", "\\newcommand{\\given}{\\,|\\,}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["$$\n", "\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n", "\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n", "\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n", "\\renewcommand{\\softmax}{\\operatorname{softmax}}\n", "\\renewcommand{\\Prob}{\\Pr}\n", "\\renewcommand{\\given}{\\,|\\,}\n", "$$"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "qyLRFIJ0pWug"}, "source": ["# Project 4: Semantic Parsing for Question Answering\n", "\n", "The goal of semantic parsing is to convert natural language utterances to a meaning representation such as a _logical form_ expression or a _SQL query_. In the previous project segment, you built a parsing system to reconstruct parse trees from the natural-language queries in the ATIS dataset. However, that only solves an intermediary task, not the end-user task of obtaining answers to the queries.\n", "\n", "In this the final project segment, you will go further, building a semantic parsing system to convert the English queries to SQL queries, so that by consulting a database you will be able to answer those questions. You will implement both a rule-based approach and an end-to-end sequence-to-sequence (seq2seq) approach. Both algorithms come with their pros and cons, and by the end of this segment you should have a basic understanding of the characteristics of the rule-based computational linguistic approach and the neural approach. \n", "\n", "## Goals\n", "\n", "1. Build a semantic parsing algorithm to convert text to SQL queries based on the syntactic parse trees from the last project.\n", "2. Build an end-to-end seq2seq system to convert text to SQL.\n", "3. Discuss the pros and cons of the rule-based system and the end-to-end system.\n", "\n", "This will be a very challenging homework, so we recommend that you start early."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "IZC1IGmoujpw"}, "source": ["## Setup"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import collections\n", "import pprint\n", "import re\n", "import warnings\n", "\n", "import nltk\n", "import sqlite3\n", "import torch\n", "import torch.nn as nn\n", "import torchtext as tt\n", "\n", "import scripts.transform as xform\n", "\n", "from cryptography.fernet import Fernet\n", "from nltk import treetransforms\n", "from nltk.corpus import treebank\n", "from nltk.grammar import ProbabilisticProduction, CFG, PCFG, induce_pcfg, Nonterminal, Production\n", "from nltk.parse import pchart\n", "from nltk.tree import Tree\n", "from torch.nn.utils.rnn import pack_padded_sequence as pack\n", "from tqdm import tqdm"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "U1jEUUYXpU0V"}, "outputs": [], "source": ["# Set random seeds\n", "seed = 1234\n", "torch.manual_seed(seed)\n", "\n", "# GPU check, make sure to set runtime type to \"GPU\" where available\n", "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "print (device)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["## Download needed scripts and data\n", "source_url = \"https://raw.githubusercontent.com/nlp-course/data/master\"\n", "\n", "...\n", "# Grammar to augment for this segment\n", "if not os.path.isfile('data/grammar'):\n", "  shell(f\"wget -nv -N -P data {source_url}/ATIS/grammar_distrib4.crypt\")\n", "\n", "  # Decrypt the grammar file\n", "  key = b'bfksTY2BJ5VKKK9xZb1PDDLaGkdu7KCDFYfVePSEfGY='\n", "  fernet = Fernet(key)\n", "  with open('./data/grammar_distrib4.crypt', 'rb') as f:\n", "    restored = Fernet(key).decrypt(f.read())\n", "  with open('./data/grammar', 'wb') as f:\n", "    f.write(Fernet(key).encrypt(restored))\n", "\n", "# Download scripts and ATIS database\n", "shell(f\"\"\"\n", "  wget -nv -N -P scripts {source_url}/scripts/trees/transform.py\n", "  wget -nv -N -P data {source_url}/ATIS/atis_sqlite.db\n", "  \"\"\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Semantically augmented grammars\n", "\n", "In the first part of this project segment, you'll be implementing a rule-based system for semantic interpretation of sentences. Before jumping into using such a system on the ATIS dataset \u2013 we'll get to that soon enough \u2013 let's first work with some trivial examples to get things going.\n", "\n", "The fundamental idea of rule-based semantic interpretation is the rule of compositionality, that **the meaning of a constituent is a function of the meanings of its immediate subconstituents and the syntactic rule that combined them**. This leads to an infrastructure for specifying semantic interpretation in which each syntactic rule in a grammar (in our case, a context-free grammar) is associated with a semantic rule that applies to the meanings associated with the elements on the right-hand side of the rule.\n", "\n", "As a first example, let's consider a grammar for arithmetic expressions, familiar from lab 3-1. Instead of reconstructing the grammar using `nltk.Grammar.from_string`, we've provided a slightly more sophisticated specification using the function `xform.parse_augmented_grammar`. You can read more about it in the file `transform.py`, which we downloaded above."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["arithmetic_grammar, arithmetic_augmentations = xform.parse_augmented_grammar(\n", "    \"\"\"\n", "    ## Sample grammar for arithmetic expressions\n", "    \n", "    S -> NUM                              : lambda Num: Num\n", "       | S OP S                           : lambda S1, Op, S2: Op(S1, S2)\n", "\n", "    OP -> ADD                             : lambda Op: Op\n", "        | SUB \n", "        | MULT\n", "        | DIV\n", "\n", "    NUM -> 'zero'                         : lambda: 0\n", "         | 'one'                          : lambda: 1\n", "         | 'two'                          : lambda: 2\n", "         | 'three'                        : lambda: 3\n", "         | 'four'                         : lambda: 4\n", "         | 'five'                         : lambda: 5\n", "         | 'six'                          : lambda: 6\n", "         | 'seven'                        : lambda: 7\n", "         | 'eight'                        : lambda: 8\n", "         | 'nine'                         : lambda: 9\n", "         | 'ten'                          : lambda: 10\n", "\n", "    ADD -> 'plus' | 'added' 'to'          : lambda: lambda x, y: x + y\n", "    SUB -> 'minus'                        : lambda: lambda x, y: x - y\n", "    MULT -> 'times' | 'multiplied' 'by'   : lambda: lambda x, y: x * y\n", "    DIV -> 'divided' 'by'                 : lambda: lambda x, y: x / y\n", "    \"\"\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["There are several things to note in this grammar specification format:\n", "\n", "1. Comment lines can be added, starting with `#`.\n", "2. Blank lines are ignored.\n", "3. Alternative right-hand sides can be split onto successive lines that start with `|`.\n", "4. Each rule can be given an _augmentation_, which is an arbitrary Python expression.\n", "5. Rules that are not explicitly provided with an augmentation (like all the `OP` rules after the first) are associated with the textually most recent one.\n", "\n", "The `parse_augmented_grammar` function returns both an NLTK grammar and a dictionary that maps from productions in the grammar to their associated augmentations. Let's examine the returned grammar."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import inspect\n", "for production in arithmetic_grammar.productions():\n", "  print(f\"{repr(production):25}     {arithmetic_augmentations[production]}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can parse with the grammar using one of the built-in NLTK parsers."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["arithmetic_parser = nltk.parse.BottomUpChartParser(arithmetic_grammar)\n", "parses = [p for p in arithmetic_parser.parse('three plus one times four'.split())]\n", "for parse in parses:\n", "  parse.pretty_print()\n", "  print(parse)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now let's turn to the augmentations. Although augmentations can be arbitrary Python expressions evaluating to arbitrary Python values, the intention is that they will often be functions that are intended to be applied to the values associated with the right-hand side elements. This enables a simple inductive evaluation of trees according to the following pseudo-code:\n", "```\n", "to evaluate a tree:\n", "  evaluate each of the nonterminal-rooted subtrees\n", "  find the augmentation associated with the root production of the tree\n", "    (it should be a function of as many arguments as there are nonterminals on the right-hand side)\n", "  return the result of applying the augmentation to the subtree values\n", "```\n", "(The base case of this recursion occurs when the number of nonterminal-rooted subtrees is zero, that is, a rule all of whose right-hand side elements are terminals. We'll have more to say about dealing with terminal symols shortly.)\n", "\n", "Suppose we had such a function, call it `eval-tree`. How would it operate on, for instance, the tree `(S (S (NUM three)) (OP (ADD plus)) (S (NUM one)))`?\n", "\n", "```\n", "evaluate (S (S (NUM three)) (OP (ADD plus)) (S (NUM one)))\n", "    evaluate (S (NUM three))\n", "        evaluate (NUM three)\n", "            (no subconstituents to evaluate)\n", "            apply the augmentation for the rule NUM -> three to the empty set of values\n", "            (lambda: 3) () ==> 3\n", "        ==> 3\n", "    evaluate (OP (ADD plus)) \n", "        ...\n", "        ==> lambda x, y: x + y\n", "    evaluate (S (NUM one))\n", "        ...\n", "        ==> 1\n", "    apply the augmentation for the rule S -> S OP S to the values 3, (lambda x, y: x + y), and 1\n", "        (lambda S1, Op, S2: Op(S1, S2))(3, (lambda x, y: x + y), 1) ==> 4\n", "    ==> 4\n", "```\n", "\n", "Thus, the string \"three plus one\" is semantically interpreted as the value 4.\n", "\n", "Now, all you need to do is to write the `eval_tree` function."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# TODO -- Write the `eval_tree` function\n", "def eval_tree(tree, grammar, augmentations):\n", "  \"\"\"Returns the value associated with the `tree` by inductive evaluation \n", "     according to the `grammar` and its `augmentations` \"\"\"\n", "  ..."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we should be able to evaluate the arithmetic example from above."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["eval_tree(parses[0], arithmetic_grammar, arithmetic_augmentations)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["And we can even write a function that parses and interprets a string. We'll have it evaluate each of the possible parses and print the results."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def interpret(string, grammar, augmentations):\n", "  parser = nltk.parse.BottomUpChartParser(grammar)\n", "  parses = parser.parse(string.split())\n", "  for parse in parses:\n", "    print(parse, \"==>\", eval_tree(parse, grammar, augmentations))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["interpret(\"three plus one times four\", arithmetic_grammar, arithmetic_augmentations)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Before going on, it will be useful to have a few more conveniences in writing augmentations for rules. First, since the augmentations are arbitrary Python expressions, they can be built from and make use of other functions. For instance, you'll notice that many of the augmentations at the leaves of the tree took no arguments and returned a constant. We can define a function `constant` that returns a function that ignores its arguments and returns a particular value."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def constant(value):\n", "  return lambda *args: value"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Similarly, several of the augmentations are functions that just return their first argument. Again, we can define a generic form `first` of such a function:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def first(*args):\n", "  return args[0]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can now rewrite the grammar above to take advantage of these shortcuts. \n", "\n", "> In the call to `parse_augmented_grammar` below, we pass in the global environment, extracted via a `globals` function call, via the argument `globals`. This allows the `parse_augmented_grammar` function to make use of the global bindings for `constant`, `first`, and the like when evaluating the augmentation expressions to their values. You can check out the code in `transform.py` to see how the passed in `globals` bindings are used."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["arithmetic_grammar_2, arithmetic_augmentations_2 = xform.parse_augmented_grammar(\n", "    \"\"\"\n", "    ## Sample grammar for arithmetic expressions\n", "    \n", "    S -> NUM                              : first\n", "       | S OP S                           : lambda S1, Op, S2: Op(S1, S2)\n", "\n", "    OP -> ADD                             : first\n", "       | SUB \n", "       | MULT\n", "       | DIV\n", "\n", "    NUM -> 'zero'                         : constant(0)\n", "         | 'one'                          : constant(1)\n", "         | 'two'                          : constant(2)\n", "         | 'three'                        : constant(3)\n", "         | 'four'                         : constant(4)\n", "         | 'five'                         : constant(5)\n", "         | 'six'                          : constant(6)\n", "         | 'seven'                        : constant(7)\n", "         | 'eight'                        : constant(8)\n", "         | 'nine'                         : constant(9)\n", "         | 'ten'                          : constant(10)\n", "\n", "    ADD -> 'plus' | 'added' 'to'          : constant(lambda x, y: x + y)\n", "    SUB -> 'minus'                        : constant(lambda x, y: x - y)\n", "    MULT -> 'times' | 'multiplied' 'by'   : constant(lambda x, y: x * y)\n", "    DIV -> 'divided' 'by'                 : constant(lambda x, y: x / y)\n", "    \"\"\",\n", "    globals=globals())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Finally, it might be occasionally useful to allow the augmentations to get ahold of the terminal symbols in the rule, in addition to the nonterminal-subtree values already available to it. Since it's only occasionally useful, we might place the list of right-hand-side terminal symbols in a named argument `terms`. This would allow us to write, for example,"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def numeric(*args, terms):\n", "  return {'zero':0, 'one':1, 'two':2, 'three':3, 'four':4, 'five':5,\n", "          'six':6, 'seven':7, 'eight':8, 'nine':9, 'ten':10}[terms[0]]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["and further simplify the grammar specification:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["arithmetic_grammar_3, arithmetic_augmentations_3 = xform.parse_augmented_grammar(\n", "    \"\"\"\n", "    ## Sample grammar for arithmetic expressions\n", "    \n", "    S -> NUM                              : first\n", "       | S OP S                           : lambda S1, Op, S2: Op(S1, S2)\n", "\n", "    OP -> ADD                             : first\n", "       | SUB \n", "       | MULT\n", "       | DIV\n", "\n", "    NUM -> 'zero'  | 'one'   | 'two'      : numeric\n", "         | 'three' | 'four'  | 'five'\n", "         | 'six'   | 'seven' | 'eight'\n", "         | 'nine'  | 'ten'\n", "\n", "    ADD -> 'plus' | 'added' 'to'          : constant(lambda x, y: x + y)\n", "    SUB -> 'minus'                        : constant(lambda x, y: x - y)\n", "    MULT -> 'times' | 'multiplied' 'by'   : constant(lambda x, y: x * y)\n", "    DIV -> 'divided' 'by'                 : constant(lambda x, y: x / y)\n", "    \"\"\",\n", "    globals=globals())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["interpret(\"six divided by three\", arithmetic_grammar_3, arithmetic_augmentations_3)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Another simple example\n", "\n", "This stuff is tricky, so it's useful to see more examples before jumping in the deep end. In this simple GEaH fragment grammar, we use a larger set of auxiliary functions to build the augmentations."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def constant(value):\n", "  \"\"\"Return `value`, ignoring any arguments\"\"\"\n", "  return lambda *args: value\n", "\n", "def forward(F, A):\n", "  \"\"\"Forward application: Return the application of the first argument to the second\"\"\"\n", "  return F(A)\n", "\n", "def backward(A, F):\n", "  \"\"\"Backward application: Return the application of the second argument to the first\"\"\"\n", "  return F(A)\n", "\n", "def first_term(*args, terms=[]):\n", "  \"\"\"Return the first (and perhaps only) terminal symbol on the right-hand side\"\"\"\n", "  return terms[0]\n", "\n", "def first(*args):\n", "  \"\"\"Return the value of the first (and perhaps only) subconstituent, ignoring any others\"\"\"\n", "  return args[0]\n", "\n", "def second(*args):\n", "  \"\"\"Return the value of the second subconstituent, ignoring any others\"\"\"\n", "  return args[1]\n", "\n", "def ignore(*args):\n", "  \"\"\"Return `None`, ignoring everything about the constituent. (Good as a placeholder until\n", "     a better augmentation can be devised.)\"\"\"\n", "  return None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Using these, we can build and test the grammar."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["geah_grammar_spec = \"\"\"\n", "  ## Productions\n", "  S -> NP VP          : backward\n", "  VP -> V NP          : forward\n", "\n", "  ## Lexicon\n", "  V -> 'likes'        : constant(lambda Object: lambda Subject: f\"likes({Object})({Subject})\")\n", "  NP -> 'Sam' | 'sam' : first_term\n", "  NP -> 'ham'\n", "  NP -> 'eggs'\n", "\"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["geah_grammar, geah_augmentations = xform.parse_augmented_grammar(geah_grammar_spec, \n", "                                                                 globals=globals())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["geah_parser = nltk.parse.BottomUpChartParser(geah_grammar)\n", "parses = [p for p in geah_parser.parse('Sam likes ham'.split())]\n", "parses[0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(eval_tree(parses[0], geah_grammar, geah_augmentations))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now you're in a good position to understand and add augmentations to a more comprehensive grammar, say, one that parses ATIS queries and generates SQL queries.\n", "\n", "In preparation for that, we need to load the ATIS data, both NL and SQL queries."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "zBALH7ZHLjhh"}, "source": ["## Loading and preprocessing the corpus\n", "\n", "To simplify things a bit, we'll only consider ATIS queries whose question type (remember that from project segment 1?) is `flight_id`. We download training, development, and test splits for this subset of the ATIS corpus, including corresponding SQL queries."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "AIVb--YGLVKd"}, "outputs": [], "source": ["# Acquire the datasets -- training, development, and test splits of the \n", "# ATIS queries and corresponding SQL queries\n", "shell(f\"\"\"\n", "  wget -nv -N -P data {source_url}/ATIS/test_flightid.nl\n", "  wget -nv -N -P data {source_url}/ATIS/test_flightid.sql\n", "  wget -nv -N -P data {source_url}/ATIS/dev_flightid.nl\n", "  wget -nv -N -P data {source_url}/ATIS/dev_flightid.sql\n", "  wget -nv -N -P data {source_url}/ATIS/train_flightid.nl\n", "  wget -nv -N -P data {source_url}/ATIS/train_flightid.sql\n", "\"\"\")"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "UW7A_icx3zjr"}, "source": ["Let's take a look at the data: the NL queries are in `.nl` files, and the SQL queries are in `.sql` files."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "Ux_vIRC337FG"}, "outputs": [], "source": ["shell(\"head -1 data/dev_flightid.nl\")\n", "shell(\"head -1 data/dev_flightid.sql\")"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "32tFSXqiLq2h"}, "source": ["### Corpus preprocessing"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "GxjjnghgCJ_b"}, "source": ["We'll use `torchtext` to process the data. We use two `Field`s: `TEXT` for the questions, and `SQL` for the SQL queries."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["## Turn off annoying torchtext warnings about pending deprecations\n", "warnings.filterwarnings(\"ignore\", module=\"torchtext\", category=UserWarning)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "ODU_4JYO8d9q"}, "outputs": [], "source": ["TEXT = tt.data.Field(lower=True,                   # lowercased\n", "                     sequential=True,              # sequential data\n", "                     include_lengths=True,         # include lengths\n", "                     batch_first=False,            # batches will be max_len X batch_size\n", "                     tokenize=lambda x: x.split(), # use split to tokenize\n", "                     preprocessing=lambda lst: list(reversed(lst))) # reverse the text before padding\n", "SQL = tt.data.Field(sequential=True,\n", "                    include_lengths=False,\n", "                    batch_first=False,\n", "                    tokenize=lambda x: x.split(),\n", "                    init_token=\"<bos>\",            # prepend <bos>\n", "                    eos_token=\"<eos>\")             # append <eos>\n", "fields = [('text_reversed', TEXT), ('sql', SQL)]"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "PRXJ17mU99V0"}, "source": ["> Note that we reversed the tokens in question using the `preprocessing` argument. We did that because in seq2seq (without attention) this trick improves performance. You can refer to Section 3.3 in [the seminal seq2seq paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43155.pdf) for more details. We also specify `batch_first=False`, so that the returned batched tensors would be of size `max_length X batch_size`, which facilitates seq2seq implementation.\n", "\n", "Now, we load the data using `torchtext`. We use the `TranslationDataset` class here because our task is essentially a translation task: \"translating\" questions into the corresponding SQL queries. Therefore, we also refer to the questions as the _source_ side and the SQL queries as the _target_ side."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "pIb_oHI33ga-"}, "outputs": [], "source": ["# Make splits for data\n", "train_data, val_data, test_data = tt.datasets.TranslationDataset.splits(\n", "    ('_flightid.nl', '_flightid.sql'), fields, path='./data/',\n", "    train='train', validation='dev', test='test')\n", "\n", "MIN_FREQ = 3\n", "TEXT.build_vocab(train_data.text_reversed, min_freq=MIN_FREQ)\n", "SQL.build_vocab(train_data.sql, min_freq=MIN_FREQ)\n", "\n", "print (f\"Size of English vocab: {len(TEXT.vocab)}\")\n", "print (f\"Most comman English words: {TEXT.vocab.freqs.most_common(10)}\\n\")\n", "\n", "print (f\"Size of SQL vocab: {len(SQL.vocab)}\")\n", "print (f\"Most comman SQL words: {SQL.vocab.freqs.most_common(10)}\\n\")\n", "\n", "print (f\"Index for start of sequence token: {SQL.vocab.stoi[SQL.init_token]}\")\n", "print (f\"Index for end of sequence token: {SQL.vocab.stoi[SQL.eos_token]}\")"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "y89Bvq7WJbc8"}, "source": ["Next, we batch our data to facilitate processing on a GPU. Batching is a bit tricky because the source and target will typically be of different lengths. Fortunately, `torchtext` allows us to pass in a `sort_key` function. By sorting on length, we can minimize the amount of padding on the source side, but since there is still some padding, we need to handle them with [`pack`](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence) later on in the seq2seq part. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "lnoY1oIj37bC"}, "outputs": [], "source": ["BATCH_SIZE = 32 # batch size for training/validation\n", "TEST_BATCH_SIZE = 1 # batch size for test, we use 1 to make implementation easier\n", "\n", "train_iter, val_iter = tt.data.BucketIterator.splits((train_data, val_data),\n", "                                                     batch_size=BATCH_SIZE, \n", "                                                     device=device,\n", "                                                     repeat=False, \n", "                                                     sort_key=lambda x: len(x.text_reversed), \n", "                                                     sort_within_batch=True)\n", "test_iter = tt.data.BucketIterator(test_data, \n", "                                   batch_size=1, \n", "                                   device=device,\n", "                                   repeat=False, \n", "                                   sort=False, \n", "                                   train=False)"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "kVzpFHVRKM1k"}, "source": ["Let's look at a single batch from one of these iterators."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "SeEpkHdiKVYV"}, "outputs": [], "source": ["batch = next(iter(train_iter))\n", "text, text_lengths = batch.text_reversed\n", "print (f\"Size of text batch: {text.shape}\")\n", "print (f\"Third sentence in batch: {text[:, 2]}\")\n", "print (f\"Length of the third sentence in batch: {text_lengths[2]}\")\n", "print (f\"Converted back to string: {' '.join([TEXT.vocab.itos[i] for i in text[:, 2]])}\")\n", "\n", "sql = batch.sql\n", "print (f\"Size of sql batch: {sql.shape}\")\n", "print (f\"Third label in batch: {sql[:, 2]}\")\n", "print (f\"Converted back to string: {' '.join([SQL.vocab.itos[i] for i in sql[:, 2]])}\")"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "LZe96xm-tx2-"}, "source": ["Note that the question is reversed, and that the size of the batch is `max_length X batch_size`. \n", "\n", "Alternatively, we can directly iterate over the raw examples in `train_data`, `val_data`, and `test_data`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "V6yIqRmwt2FA"}, "outputs": [], "source": ["for example in train_iter.dataset: # val_iter.dataset is just val_data\n", "  text_reversed = example.text_reversed\n", "  text = ' '.join(reversed(text_reversed)) # detokenized question\n", "  sql = ' '.join(example.sql) # detokenized sql\n", "  print (f\"Question: {text}\")\n", "  print (f\"SQL: {sql}\")\n", "  break"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "Vp_CdmJC5ZY2"}, "source": ["## Establishing a SQL database for evaluating ATIS queries\n", "\n", "The output of our systems will be SQL queries. How to determine if the generated queries are correct? We can't merely compare against the gold SQL queries, since there are many ways to implement a SQL query that answers any given NL query.\n", "\n", "Instead, we will execute the queries \u2013 both the predicted SQL query and the gold SQL query \u2013 on an actual database, and verify that the returned responses are the same. For that purpose, we need a SQL database server to use. We'll set one up here, using the [Python `sqlite3` module](https://docs.python.org/3.8/library/sqlite3.html)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["conn = sqlite3.connect('data/atis_sqlite.db')  # establish the DB based on the downloaded data\n", "c = conn.cursor()                              # build a \"cursor\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["To run a query, we use the cursor's `execute` function, and retrieve the results with `fetchall`. Let's get all the flights that arrive at General Mitchell International \u2013 the query above. There's a lot, so we'll just print out the first few."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["c.execute(sql)\n", "predicted_ret = c.fetchall()\n", "\n", "pprint.pprint(predicted_ret[:10])\n", "len(predicted_ret)"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "0Mri4_xjCecY"}, "source": ["## Rule-based parsing and interpretation of ATIS queries\n", "\n", "First, we will implement a rule-based semantic parser using a grammar like the one you completed in the third project segment. We've placed an initial grammar in the file `data/grammar`. We can build a parser with it:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["atis_grammar, atis_augmentations = xform.read_augmented_grammar('data/grammar')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["and parse the same sample query:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["atis_parser = nltk.parse.BottomUpChartParser(atis_grammar)\n", "parses = [p for p in atis_parser.parse(text.split())]\n", "print(text)\n", "print(\"Number of parses:\", len(parses))\n", "for parse in parses:\n", "  parse.pretty_print()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's check the coverage of this grammar on the training set."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Check coverage on training set\n", "parsed = 0\n", "with open(\"data/train_flightid.nl\") as train:\n", "  examples = train.readlines()[:]\n", "for sentence in tqdm(examples):\n", "  try:\n", "    if len(list(atis_parser.parse(sentence.strip().split()))) > 0:\n", "      parsed += 1\n", "    else:\n", "      next\n", "  except:\n", "    pass\n", "\n", "print(f\"Parsed {parsed} of {len(examples)} ({parsed/(len(examples))}%)\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The grammar that we've provided is able to parse about half of the queries in the training set."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "-0MhfJ8jYX4s"}, "source": ["## Semantic interpretation for the ATIS grammar\n", "\n", "# ***To be rewritten***\n", "\n", "Recall that the in rule-based semantic parsing each syntactic rule is associated with a semantic rule. Given a sentence, we first construct its parse tree using the syntactic rules, then compose the corresponding semantic rules bottom-up, until eventually we arrive at the root node with a finished SQL statement. \n", "\n", "We use the above parse tree as an example. \n", "\n", "1. First, let the rule\n", "\n", "   **FLIGHT -> flights**\n", "\n", "   be accompanied by the semantic rule:\n", "\n", "   **SELECT DISTINCT flight.flight_id FROM flight**.\n", "\n", "\n", "2. To handle origin/destination constraint 'boston', we associate\n", "\n", "   **Place -> boston**\n", "\n", "   with\n", "\n", "   **(SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN (SELECT city.city_code FROM city WHERE city.city_name = 'boston'))**.\n", "\n", "   > Note that we look up the airport code instead of directly using the city code, because the flight table which we later use expects the airport code.\n", "\n", "3. To distinguish destination from origin, we need to add a rule for: \n", "\n", "   **PPLACE -> to**. \n", "\n", "   We use lambda calculus here, since the SQL statement it produces is dependent on its siblings ('to boston' is different from 'to dallas'):\n", "\n", "   **$\\lambda$ x. \"(flight.to_airport IN (\" + x + \"))\"**.\n", "\n", "\n", "4. Now we need to merge *PPLACE* and *PLACE* at node *PP*:\n", "\n", "   **PP -> PPLACE PLACE**. \n", "\n", "   We simply use:\n", "\n", "   **left_child(right_child)**, \n", "\n", "   which denotes evaluating the left child to get a function, then applying that function with the right child as the input. In this case, this would evaluate to:\n", "\n", "   *(flight.to_airport IN (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN (SELECT city.city_code FROM city WHERE city.city_name = 'boston')))*\n", "\n", "\n", "5. For the rule\n", "\n", "   **PPS -> PP**,\n", "\n", "   we simply copy the evaluation result of the child:\n", "\n", "   **child**.\n", "\n", "6. Finally, the last piece to complete the puzzle is at the root node:\n", "\n", "   **S -> FLIGHT PPS**,\n", "\n", "   for which we only need to join the evaluation results of its left child and right child with a 'WHERE':\n", "\n", "   **left_child WHERE right_child**.\n", "\n", "Putting all these together, the final SQL statement we get (at root 'S') is:\n", "\n", "*SELECT DISTINCT flight.flight_id FROM flight WHERE (flight.to_airport IN (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN (SELECT city.city_code FROM city WHERE city.city_name = 'boston')))*,\n", "\n", "which should return the answer to the original question when used to query a MySQL database containing relevant flight information."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In the ATIS grammar that we provide, as with the earlier toy grammars, the augmentation for a rule with $n$ nonterminals and $m$ terminals on the right-hand side is assumed to be called with $n$ positional arguments (the values for the corresponding children), and a keyword argument `terms` whose value is a list of the $m$ terminal symbols. The `eval_tree` function you've already defined should therefore work well with this grammar.\n", "\n", "It makes use of a broader set of auxiliary functions as defined below."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Some useful auxiliary functions\n", "def constant(value):\n", "  return lambda *args: value\n", "\n", "def forward(F, A):\n", "  return F(A)\n", "\n", "def backward(A, F):\n", "  return F(A)\n", "\n", "def first_term(*args, terms=[]):\n", "  return terms[0].upper()\n", "\n", "def first(*args):\n", "  return args[0]\n", "\n", "def second(*args):\n", "  return args[1]\n", "\n", "def ignore(*args):\n", "  return None\n", "\n", "def concat(*args):\n", "  return ' '.join(filter(None, args))\n", "\n", "def select_flight():\n", "  return lambda condition: f\"\"\"\n", "    SELECT DISTINCT flight.flight_id FROM flight WHERE\n", "      {condition}\n", "    \"\"\".strip()\n", "\n", "def no_change(*args, **kwargs):\n", "  return lambda condition: condition\n", "\n", "def either_condition(cond1, cond2):\n", "   return lambda condition: f\"\"\"\n", "     (f{cond1('True')} OR f{cond2('True')}) AND {condition}\n", "     \"\"\".strip()\n", "\n", "def both_condition(cond1, cond2):\n", "   return lambda condition: cond1(cond2(condition))\n", "\n", "def from_city(city):\n", "  return lambda condition: f\"\"\"\n", "    (flight.from_airport IN\n", "     (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN\n", "        (SELECT city.city_code FROM city WHERE city.city_name = '{city}'))) AND\n", "    {condition}\n", "    \"\"\".strip()\n", "\n", "def to_city(city):\n", "  return lambda condition: f\"\"\"\n", "    (flight.to_airport IN\n", "     (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN\n", "        (SELECT city.city_code FROM city WHERE city.city_name = '{city}'))) AND\n", "    {condition}\n", "    \"\"\".strip()\n", "\n", "def airline(aircode):\n", "  return lambda condition: f\"\"\"\n", "    (flight.airline_code = '{aircode}') AND\n", "    {condition}\n", "    \"\"\".strip()\n", "\n", "def weekday(day):\n", "  return lambda condition: f\"\"\"\n", "    (flight.flight_days IN (SELECT days.days_code FROM days WHERE days.day_name = '{day.upper()}')) AND \n", "    {condition}\n", "    \"\"\".strip()\n", "\n", "def depart_around(time):\n", "  return lambda condition: f\"\"\"\n", "    (flight.departure_time >= {add_delta(miltime(time), -15).strftime('%H%M')}\n", "     AND flight.departure_time <= {add_delta(miltime(time), 15).strftime('%H%M')})\n", "    AND {condition}\n", "    \"\"\".strip()\n", "\n", "def depart_at(time):\n", "  return lambda condition: f\"\"\"\n", "    (flight.departure_time = {miltime(time).strftime('%H%M')})\n", "    AND {condition}\n", "    \"\"\".strip()\n", "\n", "def depart_before(time):\n", "  return lambda condition: f\"\"\"\n", "    (flight.departure_time <= {miltime(time).strftime('%H%M')})\n", "    AND {condition}\n", "    \"\"\".strip()\n", "\n", "def depart_after(time):\n", "  return lambda condition: f\"\"\"\n", "    (flight.departure_time >= {miltime(time).strftime('%H%M')})\n", "    AND {condition}\n", "    \"\"\".strip()\n", "\n", "def arrive_around(time):\n", "  return lambda condition: f\"\"\"\n", "    (flight.arrival_time >= {add_delta(miltime(time), -15).strftime('%H%M')}\n", "     AND flight.arrival_time <= {add_delta(miltime(time), 15).strftime('%H%M')})\n", "    AND {condition}\n", "    \"\"\".strip()\n", "\n", "def arrive_at(time):\n", "  return lambda condition: f\"\"\"\n", "    (flight.arrival_time = {miltime(time).strftime('%H%M')})\n", "    AND {condition}\n", "    \"\"\".strip()\n", "\n", "def arrive_before(time):\n", "  return lambda condition: f\"\"\"\n", "    (flight.arrival_time <= {miltime(time).strftime('%H%M')})\n", "    AND {condition}\n", "    \"\"\".strip()\n", "\n", "def arrive_after(time):\n", "  return lambda condition: f\"\"\"\n", "    (flight.arrival_time >= {miltime(time).strftime('%H%M')})\n", "    AND {condition}\n", "    \"\"\".strip()\n", "\n", "def on_date(year=1991, month=4, day=1):\n", "  return lambda condition: f\"\"\"\n", "    (flight.flight_days IN \n", "       (SELECT days.days_code FROM days\n", "          WHERE days.day_name IN \n", "          (SELECT date_day.day_name FROM date_day\n", "             WHERE date_day.year = {year}\n", "             AND date_day.month_number = {month}\n", "             AND date_day.day_number = {day}))\n", "     AND {condition})\n", "    \"\"\".strip()\n", "\n", "def month_name(month):\n", "  return {'JANUARY' : 1,\n", "          'FEBRUARY' : 2,\n", "          'MARCH' : 3,\n", "          'APRIL' : 4,\n", "          'MAY' : 5,\n", "          'JUNE' : 6,\n", "          'JULY' : 7,\n", "          'AUGUST' : 8,\n", "          'SEPTEMBER' : 9,\n", "          'OCTOBER' : 10,\n", "          'NOVEMBER' : 11,\n", "          'DECEMBER' : 12}[month.upper()]\n", "\n", "def miltime(minutes):\n", "  return datetime.time(hour=int(minutes/100), minute=(minutes % 100))\n", "\n", "def add_delta(tme, delta):\n", "    # transform to a full datetime first\n", "    return (datetime.datetime.combine(datetime.date.today(), tme) + \n", "            datetime.timedelta(minutes=delta)).time()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["atis_grammar, atis_augmentations = xform.read_augmented_grammar(\"data/grammar\", \n", "                                                                globals=globals())\n", "atis_parser = nltk.parse.SteppingChartParser(atis_grammar, strategy=nltk.parse.chart.BU_STRATEGY, trace=0)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from nltk.tokenize import word_tokenize\n", "\n", "def parse_tree(sentence):\n", "  \"\"\"Parse a sentence and return the parse tree, or None if failure.\"\"\"\n", "  try:\n", "    parses = list(atis_parser.parse(word_tokenize(sentence.lower())))\n", "    if len(parses) == 0:\n", "      return None\n", "    else:\n", "      return parses[0]\n", "  except Exception as e:\n", "    if DEBUG:\n", "      print(f\"Error: {e}\")\n", "    return None"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["parse = parse_tree('tuesday flights')\n", "parse"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["testing = eval_tree(parse, atis_grammar, atis_augmentations)\n", "print(testing)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["c.execute(testing)\n", "c.fetchall()[:10]"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "qF12SmJbDsYb"}, "source": ["### Goal 1: Construct SQL queries from a parse tree and evaluate the results"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "5iqruasFCaws"}, "source": ["Implement a rule-based semantic parsing system to successfully answer **at least 25%** of flight_id type questions in the test set."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "zoConst9BQSg"}, "source": ["#### Evaluation\n", "\n", "With a rule-based semantic parsing system, we can generate SQL queries given questions, and then execute those queries on a MySQL database to answer the given questions. To evaluate the performance of the system, we compare the returned results against the results of executing the ground truth queries. Note that we do not directly compare the predicted SQL queries to the gold SQL queries due to there being multiple ways of writing semantically equivalent queries."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "3T3b43eHBTts"}, "source": ["We provide a function `evaluate_accuracy` to compare the results from our generated SQL to the ground truth SQL."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "zMGUCce9BXEd"}, "outputs": [], "source": ["def evaluate_accuracy(predictions, sqls, questions=None):\n", "  \"\"\"\n", "  Evaluate accuracy by executing predictions on a remote MySQL database\n", "  and comparing returned results.\n", "  Arguments:\n", "      predictions: a list of predicted sqls or a single predicted sql.\n", "      sqls: a list of gold sql statements or a single gold sql.\n", "      questions: a list of questions or a single question. Optional.\n", "  Returns: accuracy.\n", "  \"\"\"\n", "  # Initial check for type of input\n", "  sqls = [sqls] if not isinstance(sqls, (list)) else sqls\n", "  predictions = [predictions] if not isinstance(predictions, (list)) else predictions\n", "  if questions is not None:\n", "    questions = [questions] if not isinstance(questions, (list)) else questions\n", "  else:\n", "    questions = ['N/A',] * len(sqls)\n", "  \n", "  # Connect to database\n", "  try:\n", "    conn = sqlite3.connect('data/atis_sqlite.db')\n", "  except Exception as err:\n", "    print(f\"Something went wrong in establishing DB: {err}\")\n", "    return\n", "\n", "  c = conn.cursor()\n", "  #c.execute('USE atis;')\n", "\n", "  # Evaluate each query and compare results\n", "  correct = 0\n", "  total = len(sqls)\n", "  for gold_sql, predicted_sql, question in zip(sqls, predictions, questions):\n", "    is_correct = True\n", "    if len(predicted_sql) == 0:\n", "      is_correct = False\n", "    else:\n", "      # Execute predicted sql\n", "      try:\n", "        c.execute(predicted_sql)\n", "        predicted_ret = c.fetchall()\n", "      except Exception as e:\n", "        predicted_ret = 'Syntax Error!'\n", "      # Execute gold sql\n", "      try:\n", "        c.execute(gold_sql)\n", "        gold_ret = c.fetchall()\n", "      except Exception as e:\n", "        gold_ret = 'Syntax Error!'\n", "      \n", "      if gold_ret == predicted_ret:\n", "        correct += 1\n", "      else:\n", "        is_correct = False\n", "    if DEBUG and not is_correct:\n", "      print (f\"\\nINCORRECT!\")\n", "      print (f\"Question: {question}\")\n", "      print (f\"Gold SQL: {gold_sql}\")\n", "      if len(gold_sql) > 0:\n", "        print (f\"Gold Result: {gold_ret[:50]}\")\n", "      print (f\"Predicted SQL: {predicted_sql}\")\n", "      if len(predicted_sql) > 0:\n", "        print (f\"Predicted Result: {predicted_ret[:50]}\")\n", "  \n", "  conn.commit()\n", "  c.close()\n", "  conn.close()\n", "  return correct/total"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "IPR4zSqzM58Z"}, "source": ["To make development faster, we recommend starting with a few examples before running the full evaluation script."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["DEBUG = False"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "JZNW0pWxBiyj"}, "outputs": [], "source": ["# Example 1\n", "question = 'flights from phoenix to milwaukee'\n", "gold_sql = \"SELECT DISTINCT flight_1.flight_id FROM flight flight_1 , airport_service airport_service_1 , city city_1 , airport_service airport_service_2 , city city_2 WHERE flight_1.from_airport = airport_service_1.airport_code AND airport_service_1.city_code = city_1.city_code AND city_1.city_name = 'PHOENIX' AND flight_1.to_airport = airport_service_2.airport_code AND airport_service_2.city_code = city_2.city_code AND city_2.city_name = 'MILWAUKEE'\"\n", "tree = parse_tree(question)\n", "tree.pretty_print()\n", "\n", "predicted_sql = eval_tree(tree, atis_grammar, atis_augmentations)\n", "print (f\"Accuracy: {evaluate_accuracy(predicted_sql, gold_sql, question)}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "_ER7QlkPK4pg"}, "outputs": [], "source": ["# Example 2\n", "question = 'i would like a united flight'\n", "gold_sql = \"SELECT DISTINCT flight_1.flight_id FROM flight flight_1 WHERE flight_1.airline_code = 'UA'\"\n", "tree = parse_tree(question)\n", "tree.pretty_print()\n", "\n", "predicted_sql = eval_tree(tree, atis_grammar, atis_augmentations)\n", "print (f\"Accuracy: {evaluate_accuracy(predicted_sql, gold_sql, question)}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "_ER7QlkPK4pg"}, "outputs": [], "source": ["# Example 2\n", "question = 'i would like a flight between boston and dallas'\n", "gold_sql = \"SELECT DISTINCT flight.flight_id FROM flight WHERE TRUE AND (flight.from_airport IN (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN (SELECT city.city_code FROM city WHERE city.city_name = 'BOSTON'))) AND (flight.to_airport IN (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN (SELECT city.city_code FROM city WHERE city.city_name = 'DALLAS')))\"\n", "tree = parse_tree(question)\n", "tree.pretty_print()\n", "\n", "predicted_sql = eval_tree(tree, atis_grammar, atis_augmentations)\n", "print(predicted_sql)\n", "print (f\"Accuracy: {evaluate_accuracy(predicted_sql, gold_sql, question)}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "AJQ58JuHK6_S"}, "outputs": [], "source": ["# Example 3\n", "question = 'what flights are departing from houston or austin leaving at 7am sunday'\n", "gold_sql = \"SELECT DISTINCT flight.flight_id FROM flight WHERE TRUE AND TRUE AND ((flight.from_airport IN (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN (SELECT city.city_code FROM city WHERE city.city_name = 'HOUSTON'))) OR (flight.from_airport IN (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN (SELECT city.city_code FROM city WHERE city.city_name = 'AUSTIN')))) AND (flight.departure_time >= 630 AND flight.departure_time <= 730) AND (flight.flight_days IN (SELECT days.days_code FROM days WHERE days.day_name = 'sunday'))\"\n", "tree = parse_tree(question)\n", "tree.pretty_print()\n", "\n", "predicted_sql = eval_tree(tree, atis_grammar, atis_augmentations)\n", "print (f\"Accuracy: {evaluate_accuracy(predicted_sql, gold_sql, question)}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "8BW7LSCcK84G"}, "outputs": [], "source": ["# Example 4\n", "question = 'can i have a flight from san francisco that stops in dallas going to new york arriving before 6pm'\n", "gold_sql = \"SELECT DISTINCT flight.flight_id FROM flight WHERE TRUE AND (flight.from_airport IN (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN (SELECT city.city_code FROM city WHERE city.city_name = 'SAN FRANCISCO'))) AND (flight_stop.stop_airport IN (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN (SELECT city.city_code FROM city WHERE city.city_name = 'DALLAS'))) AND (flight.to_airport IN (SELECT airport_service.airport_code FROM airport_service WHERE airport_service.city_code IN (SELECT city.city_code FROM city WHERE city.city_name = 'NEW YORK'))) AND (flight.arrival_time <= 1800)\"\n", "tree = parse_tree(question)\n", "tree.pretty_print()\n", "\n", "predicted_sql = eval_tree(tree, atis_grammar, atis_augmentations)\n", "print (f\"Accuracy: {evaluate_accuracy(predicted_sql, gold_sql, question)}\")"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "xA3uatiaBX_f"}, "source": ["Below is the full evaluation code. You should be able to get correct results on at least 25% of `flight_id` type questions from the test set."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "HlO7riRGM3BT"}, "outputs": [], "source": ["questions = []\n", "predictions = []\n", "gold_sqls = []\n", "\n", "DEBUG = False\n", "for example in tqdm(test_iter.dataset):\n", "  # Input and output\n", "  text_reversed = example.text_reversed\n", "  question = ' '.join(reversed(text_reversed)) # detokenized question\n", "  gold_sql = ' '.join(example.sql) # detokenized sql\n", "  questions.append(question)\n", "  gold_sqls.append(gold_sql)\n", "  # Get parse tree\n", "  tree = parse_tree(question)\n", "  if tree is None:\n", "    predictions.append('')\n", "    continue\n", "  # Predict\n", "  try:\n", "    predicted_sql = eval_tree(tree, atis_grammar, atis_augmentations)\n", "  except Exception as e:\n", "    predictions.append('')\n", "    continue\n", "  predictions.append(predicted_sql)\n", "\n", "evaluate_accuracy(predictions, gold_sqls, questions)"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "j00wMxHP3MnB"}, "source": ["## End-to-End Seq2Seq Model"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "hE17C1s0QhSq"}, "source": ["Nowadays neural networks dominate the field of NLP research. In this part, we investigate if it is possible to use an end-to-end system to directly learn the mapping from the natural language questions to the SQL queries."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "dGvpHlNnN3ym"}, "source": ["### Goal 2: Implement a seq2seq model"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "4TPBjDalLzXg"}, "source": ["#### Model, Optimization and Decoding\n", "\n", "For the sequence-to-sequence model, you need to implement the class `EncoderDecoder`. We have provided starter code for performing optimization, but there are at least five methods that you need to implement:\n", "\n", "1. `__init__`: an initializer where you can create network modules.\n", "\n", "2. `forward`: given question word ids of size `batch_size X max_length`, question lengths of size `batch_size` and SQL word ids `batch_size X max_length_sql`, returns logits `batch_size X max_length_sql`. Note that here the batch size can be greater than 1.\n", "\n", "3. `compute_loss`: computes loss by comparing output returned by forward to ground_truth which stores the true SQL word ids.\n", "\n", "4. `evaluate_ppl`: evaluate the current model's perplexity on a given dataset iterator. [Perplexity](https://en.wikipedia.org/wiki/Perplexity) is defined as $\\exp(-\\frac{\\text{total log likelihood})}{\\text{total number of words}})$, which can be roughly understood as how many random guesses the model needs to make to get a word correct.\n", "\n", "5. `predict`: Generates the target sequence (SQL) given the source sequence (question). Note that here you can assume the batch size to be always 1 for simplicity. Besides, you can use greedy decoding here, i.e., predicting the word with the highest probability at any time step, although in practice researchers use more complicated decoding methods such as beam search. \n", "\n", "This implementation is essentially building an entire neural seq2seq system, so expect it to be very challenging. The code you write here can also be used for other seq2seq tasks such as machine translation and document summarization.\n", "\n", "*Hint: to handle source side paddings in `torch`, you can use somethine like `packed_src = pack(src, src_lengths)`. To handle target side paddings, you can use `ignore_index` when creating the loss function."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "afAaAkJXMhO7"}, "outputs": [], "source": ["#TODO\n", "class EncoderDecoder(nn.Module):\n", "  def __init__(self, text, sql, embedding_size=512, hidden_size=512, layers=2,\n", "               dropout=0, bidirectional=False, share_decoder_input_output_embeds=False,\n", "               add_encoder_out_to_decoder_input=False):\n", "    \"\"\"\n", "    Initializer. Creates network modules and loss function. You do not need to\n", "    implement all features as long as you can achieve 30%+ accuracy.\n", "    Arguments:\n", "        text: text field\n", "        tag: sql field\n", "        embedding_size: word embedding size\n", "        hidden_size: hidden layer size\n", "        layers: number of layers\n", "        dropout: dropout\n", "        bidirectional: use bidirectional RNN cells\n", "        share_decoder_input_output_embeds: if True, set the weight matrix of the \n", "            final projection layer to be the same as decoder word embeddings.\n", "            This reduces the number of parameters and is found to improve performance.\n", "            See https://arxiv.org/pdf/1608.05859.pdf.\n", "        add_encoder_out_to_decoder_input: if True, add encoder output to every\n", "            step of decoder input. This trick keeps the decoder from forgetting\n", "            encoder outputs as it decodes.\n", "    \"\"\"\n", "    super(EncoderDecoder, self).__init__()\n", "    self.text = text\n", "    self.sql = sql\n", "    # Keep the vocabulary sizes available\n", "    self.V_src = len(text.vocab.itos)\n", "    self.V_tgt = len(sql.vocab.itos)\n", "    # Get special word ids or tokens\n", "    self.padding_id_src = text.vocab.stoi[text.pad_token]\n", "    self.padding_id_tgt = sql.vocab.stoi[sql.pad_token]\n", "    self.bos_id = sql.vocab.stoi[sql.init_token]\n", "    self.eos_id = sql.vocab.stoi[sql.eos_token]\n", "    self.eos_token = sql.eos_token\n", "\n", "    # Keep parameters available\n", "    self.embedding_size = embedding_size\n", "    self.hidden_size = hidden_size\n", "    self.layers = layers\n", "    self.dropout = dropout\n", "    self.share_decoder_input_output_embeds = share_decoder_input_output_embeds\n", "    self.bidirectional = bidirectional\n", "    self.add_encoder_out_to_decoder_input = add_encoder_out_to_decoder_input\n", "\n", "    #TODO: implement this method\n", "    # Create essential modules and loss function\n", "    \"your code here\"\n", "\n", "  def forward(self, src_words, src_lengths, tgt_words):\n", "    \"\"\"\n", "    Performs forward computation, returns logits.\n", "    Arguments:\n", "        src_words: question batch of size batch_size X max_length\n", "        src_lengths: question lengths of size batch_size\n", "        tgt_words: sql batch of size batch_size X max_length\n", "    \"\"\"\n", "    #TODO: implement this method\n", "    \"your code here\"\n", "    return logits\n", "\n", "  def compute_loss(self, logits, targets):\n", "    \"\"\"\n", "    Computes loss function with logits and target.\n", "    Arguments:\n", "        logits: tensor of size batch_size X max_length X V_tgt\n", "        targets: tensor of size batch_size X max_length\n", "    \"\"\"\n", "    #TODO: implement this method\n", "    \"your code here\"\n", "    return loss\n", "\n", "  def evaluate_ppl(self, iterator):\n", "    \"\"\"\n", "    Returns the model's perplexity on a given dataset `iterator`. We will\n", "    use it for model selection.\n", "    \"\"\"\n", "    # Switch to eval mode\n", "    self.eval()\n", "    #TODO: implement this method\n", "    \"your code here\"\n", "    return perplexity\n", "\n", "  def predict(self, src_words, src_lengths, max_tgt_length=200):\n", "    \"\"\"\n", "    Generates the target sequence (SQL) given the source sequence (question).\n", "    You only need to implemnt greedy decoding, i.e., at each decoding step,\n", "    find the word with the highest probability.\n", "    Note that for simplicity, we only use batch size 1.\n", "    Arguments:\n", "        src_words: a tensor of size (max_length, 1) storing question word ids.\n", "        src_lengths: a tensor of size (1) storing question length.\n", "        max_tgt_length: at most proceed this many steps of decoding\n", "    Returns: \n", "        a string of the generated SQL.\n", "    \"\"\"\n", "    # Switch to eval mode\n", "    self.eval()\n", "    #TODO: implement this method\n", "    \"your code here\"\n", "    decoded = 'SELECT DISTINCE * FROM flight'\n", "    return decoded\n", "    \n", "  def fit(self, train_iter, val_iter, epochs=50, learning_rate=3e-4):\n", "    \"\"\"Train the model.\"\"\"\n", "    # Switch the module to training mode\n", "    self.train()\n", "    # Use Adam to optimize the parameters\n", "    optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n", "    best_validation_ppl = float('inf')\n", "    best_model = None\n", "    # Run the optimization for multiple epochs\n", "    for epoch in range(epochs): \n", "      total_words = 0\n", "      total_loss = 0.0\n", "      for batch in tqdm(train_iter):\n", "        # Zero the parameter gradients\n", "        self.zero_grad()\n", "\n", "        # Input and target\n", "        text, text_lengths = batch.text_reversed # text: max_length_text, bsz\n", "        sql = batch.sql # max_length_sql, bsz\n", "        sql_in = sql[:-1] # Remove <eos> for decode input\n", "        sql_out = sql[1:] # Remove <bos> as target\n", "        batch_size = sql.size(1)\n", "        \n", "        # Run forward pass and compute loss along the way.\n", "        logits = self.forward(text, text_lengths, sql_in)\n", "        loss = self.compute_loss(logits, sql_out)\n", "\n", "        # Training stats\n", "        num_sql_words = sql_out.ne(self.padding_id_tgt).float().sum().item()\n", "        total_words += num_sql_words\n", "        total_loss += loss.item()\n", "        \n", "        # Perform backpropagation\n", "        loss.div(batch_size).backward()\n", "        optim.step()\n", "\n", "      # Evaluate and track improvements on the validation dataset\n", "      validation_ppl = self.evaluate_ppl(val_iter)\n", "      self.train()\n", "      if validation_ppl < best_validation_ppl:\n", "        best_validation_ppl = validation_ppl\n", "        self.best_model = copy.deepcopy(self.state_dict())\n", "      epoch_loss = total_loss / total_words\n", "      print (f'Epoch: {epoch} Training Perplexity: {math.exp(epoch_loss):.4f} '\n", "             f'Validation Perplexity: {validation_ppl:.4f}')"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "PV2NuIBxOH3-"}, "source": ["#### Solution<!--Solution-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "febNSFFp4FT1"}, "outputs": [], "source": ["#Solution\n", "class EncoderDecoder(nn.Module):\n", "  def __init__(self, text, sql, embedding_size=512, hidden_size=512, layers=2,\n", "               dropout=0, bidirectional=False, share_decoder_input_output_embeds=False,\n", "               add_encoder_out_to_decoder_input=False):\n", "    \"\"\"\n", "    Initializer. Creates network modules and loss function. You do not need to\n", "    implement all features as long as you can achieve 30%+ accuracy.\n", "    Arguments:\n", "        text: text field\n", "        tag: sql field\n", "        embedding_size: word embedding size\n", "        hidden_size: hidden layer size\n", "        layers: number of layers\n", "        dropout: dropout\n", "        bidirectional: use bidirectional RNN cells\n", "        share_decoder_input_output_embeds: if True, set the weight matrix of the \n", "            final projection layer to be the same as decoder word embeddings.\n", "            This reduces the number of parameters and is found to improve performance.\n", "            See https://arxiv.org/pdf/1608.05859.pdf.\n", "        add_encoder_out_to_decoder_input: if True, add encoder output to every\n", "            step of decoder input. This trick keeps the decoder from forgetting\n", "            encoder outputs as it decodes.\n", "    \"\"\"\n", "    super(EncoderDecoder, self).__init__()\n", "    self.text = text\n", "    self.sql = sql\n", "    # Keep the vocabulary sizes available\n", "    self.V_src = len(text.vocab.itos)\n", "    self.V_tgt = len(sql.vocab.itos)\n", "    # Get special word ids or tokens\n", "    self.padding_id_src = text.vocab.stoi[text.pad_token]\n", "    self.padding_id_tgt = sql.vocab.stoi[sql.pad_token]\n", "    self.bos_id = sql.vocab.stoi[sql.init_token]\n", "    self.eos_id = sql.vocab.stoi[sql.eos_token]\n", "    self.eos_token = sql.eos_token\n", "\n", "    # Keep parameters available\n", "    self.embedding_size = embedding_size\n", "    self.hidden_size = hidden_size\n", "    self.layers = layers\n", "    self.dropout = dropout\n", "    self.share_decoder_input_output_embeds = share_decoder_input_output_embeds\n", "    self.bidirectional = bidirectional\n", "    self.add_encoder_out_to_decoder_input = add_encoder_out_to_decoder_input\n", "\n", "    # Create essential modules\n", "    self.word_embeddings_src = nn.Embedding(self.V_src, embedding_size)\n", "    self.word_embeddings_tgt = nn.Embedding(self.V_tgt, embedding_size)\n", "    self.dropout_layer = nn.Dropout(dropout)\n", "\n", "    # RNN cells\n", "    self.encoder_rnn = nn.LSTM(\n", "      input_size    = embedding_size,\n", "      hidden_size   = hidden_size//2 if bidirectional else hidden_size,\n", "      num_layers    = layers,\n", "      dropout       = dropout,\n", "      bidirectional = bidirectional\n", "    )\n", "    self.decoder_rnn = nn.LSTM(\n", "      input_size    = embedding_size,\n", "      hidden_size   = hidden_size,\n", "      num_layers    = layers,\n", "      dropout       = dropout,\n", "    )\n", "\n", "    # Final projection layer\n", "    self.hidden2output = nn.Linear(hidden_size, self.V_tgt)\n", "    if share_decoder_input_output_embeds:\n", "      self.hidden2output.weight = self.word_embeddings_tgt.weight\n", "   \n", "    # Create loss function\n", "    self.loss_function = nn.CrossEntropyLoss(reduction='sum', \n", "                                             ignore_index=self.padding_id_tgt)\n", "\n", "  def encode(self, src_words, src_lengths):\n", "    \"\"\"Encode source words into a vector\"\"\"\n", "    # Compute word embeddings\n", "    src = self.word_embeddings_src(src_words) # max_len, bsz, embedding_size\n", "    if isinstance(src_lengths, torch.LongTensor) \\\n", "            or isinstance(src_lengths, torch.cuda.LongTensor):\n", "      src_lengths = src_lengths.tolist()\n", "    # Deal with paddings\n", "    packed_src = pack(src, src_lengths)\n", "    # Forward RNN and return final state\n", "    encoder_out = self.encoder_rnn(packed_src)[-1] # num_layers*num_directions, bsz, hidden_size/num_directions\n", "    # Reshape encoder_out for bidirectional case\n", "    if self.bidirectional:\n", "      batch_size = len(src_lengths)\n", "      h, c = encoder_out\n", "      h = h.view(-1, 2, batch_size, self.hidden_size//2) \\\n", "           .transpose(1, 2) \\\n", "           .contiguous().view(-1, batch_size, self.hidden_size) # num_layers, bsz, hidden_size\n", "      c = c.view(-1, 2, batch_size, self.hidden_size//2) \\\n", "           .transpose(1, 2) \\\n", "           .contiguous().view(-1, batch_size, self.hidden_size) # num_layers, bsz, hidden_size\n", "      encoder_out = (h, c)\n", "    return encoder_out\n", "\n", "  def decode(self, tgt_words, encoder_out, feed_decoder_input):\n", "    \"\"\"Decode based on encoder output\"\"\"\n", "    # Compute word embeddings\n", "    tgt = self.word_embeddings_tgt(tgt_words) # len, bsz, hidden\n", "    # Optionally add feed_decoder_input to every step\n", "    if feed_decoder_input is not None: # bsz, hidden\n", "      tgt = tgt + feed_decoder_input.unsqueeze(0) # unsqueeze to 1, bsz, hidden\n", "    # Forward decoder RNN and return all hidden states\n", "    return self.decoder_rnn(tgt, encoder_out)[0]\n", "\n", "  def forward(self, src_words, src_lengths, tgt_words):\n", "    \"\"\"\n", "    Performs forward computation, returns logits.\n", "    Arguments:\n", "        src_words: question batch of size batch_size X max_length\n", "        src_lengths: question lengths of size batch_size\n", "        tgt_words: sql batch of size batch_size X max_length\n", "    \"\"\"\n", "    # Forward encoder\n", "    encoder_out = self.encode(src_words, src_lengths) # tuple of (h_final, c_final)\n", "    if self.share_decoder_input_output_embeds:\n", "      # h_final/c_final size: num_layers, bsz, hidden_size\n", "      # We only take the last layer to match shape of decoder inputs\n", "      feed_decoder_input = encoder_out[0][-1] + encoder_out[1][-1] # bsz, hidden_size\n", "    else:\n", "      feed_decoder_input = None\n", "    # Forward decoder\n", "    decoder_out = self.decode(tgt_words, encoder_out, feed_decoder_input)\n", "    # Final projection to target vocabulary\n", "    logits = self.hidden2output(self.dropout_layer(decoder_out))\n", "    return logits\n", "\n", "  def compute_loss(self, logits, targets):\n", "    \"\"\"\n", "    Computes loss function with logits and target.\n", "    Arguments:\n", "        logits: tensor of size batch_size X max_length X V_tgt\n", "        targets: tensor of size batch_size X max_length\n", "    \"\"\"\n", "    return self.loss_function(logits.view(-1, self.V_tgt), targets.view(-1))\n", "\n", "  def evaluate_ppl(self, iterator):\n", "    \"\"\"Returns the model's perplexity on a given dataset `iterator`.\"\"\"\n", "    # Switch to eval mode\n", "    self.eval()\n", "    total_loss = 0\n", "    total_words = 0\n", "    for batch in iterator:\n", "      # Input and target\n", "      text, text_lengths = batch.text_reversed\n", "      sql = batch.sql # max_length_sql, bsz\n", "      sql_in = sql[:-1] # remove <eos> for decode input\n", "      sql_out = sql[1:] # remove <bos> as target\n", "      # Forward to get logits\n", "      logits = self.forward(text, text_lengths, sql_in)\n", "      # Compute cross entropy loss\n", "      loss = self.compute_loss(logits, sql_out)\n", "      total_loss += loss.item()\n", "      total_words += sql_out.ne(self.padding_id_tgt).float().sum().item()\n", "    return math.exp(total_loss/total_words)\n", "\n", "  def predict(self, src_words, src_lengths, max_tgt_length=200):\n", "    \"\"\"\n", "    Generates the target sequence (SQL) given the source sequence (question).\n", "    You only need to implemnt greedy decoding, i.e., at each decoding step,\n", "    find the word with the highest probability.\n", "    Note that for simplicity, we only use batch size 1.\n", "    Arguments:\n", "        src_words: a tensor of size (max_length, 1) storing question word ids.\n", "        src_lengths: a tensor of size (1) storing question length.\n", "        max_tgt_length: at most proceed this many steps of decoding\n", "    Returns: \n", "        a string of the generated SQL.\n", "    \"\"\"\n", "    # Switch to eval mode\n", "    self.eval()\n", "    # Forward encoder\n", "    encoder_out = self.encode(src_words, src_lengths) # tuple of (h_final, c_final)\n", "    if self.share_decoder_input_output_embeds:\n", "      # h_final/c_final size: num_layers, bsz, hidden_size\n", "      # We only take the last layer to match shape of decoder inputs\n", "      feed_decoder_input = encoder_out[0][-1] + encoder_out[1][-1] # bsz, hidden_size\n", "    else:\n", "      feed_decoder_input = None\n", "    \n", "    batch_size = src_words.size(1)\n", "    # Create initial decoder input\n", "    initial_words = torch.zeros(1, batch_size, device=device).fill_(self.bos_id).long()\n", "    decoder_input = self.word_embeddings_tgt(initial_words) # 1, bsz, embedding_size\n", "    hidden = encoder_out # initialize decoder hidden state\n", "    \n", "    decoded = [] # stores partial decoding results\n", "    # Forward one step at a time\n", "    for _ in range(max_tgt_length):\n", "      # Forward decoder for one step\n", "      if self.add_encoder_out_to_decoder_input:\n", "        decoder_input = decoder_input + feed_decoder_input.unsqueeze(0)\n", "      output, hidden = self.decoder_rnn(decoder_input, hidden)\n", "      # Forward final projection\n", "      logits = self.hidden2output(self.dropout_layer(output)).squeeze(0) # bsz, vocab\n", "      # Take argmax to find the most probable word\n", "      current_words = logits.argmax(1) # bsz\n", "      # Set next step decoder inputs\n", "      words = current_words.view(1, -1)\n", "      decoder_input = self.word_embeddings_tgt(words)\n", "      # Break if eos is encountered\n", "      if current_words.item() == self.eos_id:\n", "        break\n", "      # Find the tokens\n", "      decoded.append(self.sql.vocab.itos[current_words.item()])\n", "    return ' '.join(decoded)\n", "\n", "  def fit(self, train_iter, val_iter, epochs=10, learning_rate=3e-4):\n", "    \"\"\"Train the model.\"\"\"\n", "    # Switch the module to training mode\n", "    self.train()\n", "    # Use Adam to optimize the parameters\n", "    optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n", "    best_validation_ppl = float('inf')\n", "    best_model = None\n", "    # Run the optimization for multiple epochs\n", "    for epoch in range(epochs): \n", "      total_words = 0\n", "      total_loss = 0.0\n", "      for batch in tqdm(train_iter):\n", "        # Zero the parameter gradients\n", "        self.zero_grad()\n", "\n", "        # Input and target\n", "        text, text_lengths = batch.text_reversed # text: max_length_text, bsz\n", "        sql = batch.sql # max_length_sql, bsz\n", "        sql_in = sql[:-1] # Remove <eos> for decode input\n", "        sql_out = sql[1:] # Remove <bos> as target\n", "        batch_size = sql.size(1)\n", "        \n", "        # Run forward pass and compute loss along the way.\n", "        logits = self.forward(text, text_lengths, sql_in)\n", "        loss = self.compute_loss(logits, sql_out)\n", "\n", "        # Training stats\n", "        num_sql_words = sql_out.ne(self.padding_id_tgt).float().sum().item()\n", "        total_words += num_sql_words\n", "        total_loss += loss.item()\n", "        \n", "        # Perform backpropagation\n", "        loss.div(batch_size).backward()\n", "        optim.step()\n", "\n", "      # Evaluate and track improvements on the validation dataset\n", "      validation_ppl = self.evaluate_ppl(val_iter)\n", "      self.train()\n", "      if validation_ppl < best_validation_ppl:\n", "        best_validation_ppl = validation_ppl\n", "        self.best_model = copy.deepcopy(self.state_dict())\n", "      epoch_loss = total_loss / total_words\n", "      print (f'Epoch: {epoch} Training Perplexity: {math.exp(epoch_loss):.4f} '\n", "             f'Validation Perplexity: {validation_ppl:.4f}')"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "JLuzTt-RFgoL"}, "source": ["After implementing the `EncoderDecoder` class, you can use the below script to create the model and kick off training. You are free to tune the hyperparameters."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "t4fGKij64rkQ"}, "outputs": [], "source": ["EPOCHS = 10 # epochs, we highly recommend starting with a smaller number like 1\n", "LEARNING_RATE = 3e-4 # learning rate\n", "# Instantiate and train classifier\n", "model = EncoderDecoder(TEXT, SQL,\n", "  embedding_size = 1024,\n", "  hidden_size    = 1024,\n", "  dropout        = 0.1,\n", "  layers         = 3,\n", "  bidirectional  = True,\n", "  share_decoder_input_output_embeds = True,\n", "  add_encoder_out_to_decoder_input = True,\n", ").to(device)\n", "\n", "model.fit(train_iter, val_iter, epochs=EPOCHS, learning_rate=LEARNING_RATE)\n", "model.load_state_dict(model.best_model)\n", "\n", "# Evaluate model performance, the expected value shall be < 1.3\n", "# We use validation set because this particular test set has a different distribution\n", "print (f'Validation perplexity: {model.evaluate_ppl(val_iter):.3f}')"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "dKkyL41z9ggD"}, "source": ["#### Evaluation"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "kLyrRJPsqPEs"}, "source": ["Now we are ready to run the full evaluation. For seq2seq, a proper implementation should reach at least 30% accuracy."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "vesYcMfF8XYz"}, "outputs": [], "source": ["questions = []\n", "predictions = []\n", "gold_sqls = []\n", "\n", "for example in test_iter.dataset: # val_iter.dataset is just val_data\n", "  # Input and output\n", "  text_reversed_str = example.text_reversed\n", "  question = ' '.join(list(reversed(text_reversed_str))) # detokenized question\n", "  gold_sql = ' '.join(example.sql) # detokenized sql\n", "  questions.append(question)\n", "  gold_sqls.append(gold_sql)\n", "  # Predict\n", "  text, text_lengths = TEXT.process([text_reversed_str])\n", "  text = text.to(device)\n", "  text_lengths = text_lengths.to(device)\n", "  prediction = model.predict(text, text_lengths)\n", "  print (prediction)\n", "  predictions.append(prediction)\n", "  \n", "evaluate_accuracy(predictions, gold_sqls, questions)"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "HV5Q1nuCReVv"}, "source": ["## Discussion"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "fwg23wdz1o8o"}, "source": ["### Goal 3: Compare the pros and cons of rule-based and neural approaches.\n", "\n", "Compare the pros and cons of both approaches with relevant examples from your experiments above. Concerning the accuracy, which approach would you choose to be used in a product? Explain."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "JEmuLudUbnUL"}, "source": ["#### Solution<!--Solution-->\n", "For rule-based semantic parsing, as long as the written semantic rules consider all possible cases (which is a nontrivial task), it can solve this task nicely. We list some pros and cons of this approach, but our answer is by no means exhaustive.\n", "\n", "Pros\n", "*   Clearly interpretable. When the system makes a mistake we can easily pinpoint where the problem is, and write more rules to fix it.\n", "*   Robust. For the cases that we considered, even if at test time there are examples with many constraints, the generated SQL would still be correct.\n", "*   Low sample complexity. Developing the semantic rules does not need thousands of examples. We are very good at generalization and we only used dozens of examples to write those rules in the solution.\n", "\n", "Cons\n", "*   High develop cost. It is a lot of work to develop those semantic rules.\n", "*   Poor transferability. For a new domain such as question answering in wikipedia, we need to develop a new set of rules to make this method work.\n", "\n", "For the end-to-end seq2seq approach, as long as we have enough data (which is not always the case in reality), enough model capacity (limited by hardware and time), and if the test domain is similar to the training domain, then the approach would be expeced to work well. Below lists some of its pros and cons.\n", "\n", "Pros\n", "*   High performance. With enough training data, this approach performs well as evidenced by this project.\n", "*   Low develop cost. Developing the seq2seq model is much easier compared to writing semantic rules and does not require lingustic background.\n", "*   High transferability if we have training data. For a new domain, as long as we have enough training instances, we can train the same model on the new training set to solve the problem. However, we do want to note that without training anew the model trained on one domain is unlikely to work on another.\n", "\n", "Cons\n", "*   Poor interpretability. When the model makes a mistake, there is no easy way of fixing it. The best we can do is to collect more data similar to the broken ones and add to the training set.\n", "*   High sample complexity. We need a huge training set to make this approach work. There's no way it'd work using dozens of training examples.\n", "*   Sensitive. By sensitive we meant if training set only contains compositions up to a certain level, then at test time the trained model is unlikely to work on any instance with a higher number of compositions. If we trained on sentences with length up to 100, then at test time it cannot work on sentences of length 150.\n", "\n", "Best approach:\n", "If we only care about performance, it is most natural to select the seq2seq approach due to its higher performance. Though depending on the results from precision and recall, it may be best to choose the approach with the best precision scores when applying the approach for customer use (take in the case of Alexa).\n"]}], "metadata": {"accelerator": "GPU", "colab": {"collapsed_sections": [], "include_colab_link": true, "name": "project4_semantics.ipynb", "provenance": [], "toc_visible": true}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.3"}}, "nbformat": 4, "nbformat_minor": 4}